{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### paper: \"Comparing high-dimensional neural recordings by aligning their low-dimensional latent representations\" ######\n",
    "\"\"\"\n",
    "In this paper, the authors propose a method for comparing neural recordings across different animals, \n",
    "brain regions, and tasks. The method involves mapping the high-dimensional neural recordings onto a \n",
    "low-dimensional latent space using a dimensionality reduction technique called non-negative matrix \n",
    "factorization (NMF). The authors then align the latent representations of the neural recordings using \n",
    "a technique called Procrustes analysis, which allows for comparisons between neural data from different sources.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/miniconda3/envs/deepdebugger/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET resnet18\n",
      "Finish initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 462.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET resnet18_with_dropout\n",
      "Finish initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 6411.23it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 5697.85it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 5087.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute alignment indicates number: 106 label diff indicates number: 12 confidence diff indicates number: 16 high distance number: 97\n"
     ]
    }
   ],
   "source": [
    "####### dropout resnet18 vs without dropout\n",
    "#### \n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "\n",
    "CLEAN_PATH = \"/home/yifan/dataset/resnet18_with_dropout/pairflip/cifar10/0/\"\n",
    "REF_PATH = \"/home/yifan/dataset/clean/pairflip/cifar10/0\"\n",
    "\n",
    "\n",
    "ENCODER_DIMS=[512,256,256,256,256,2]\n",
    "DECODER_DIMS= [2,256,256,256,256,512]\n",
    "VIS_MODEL_NAME = 'vis'\n",
    "\n",
    "DEVICE='cuda:1'\n",
    "########## initulize reference data and target data\n",
    "from alignment.data_preprocess import DataInit\n",
    "REF_EPOCH = 200\n",
    "TAR_EPOCH = 200\n",
    "ref_datainit = DataInit(REF_PATH,REF_PATH,REF_EPOCH)\n",
    "tar_datainit = DataInit(CLEAN_PATH,CLEAN_PATH,TAR_EPOCH)\n",
    "ref_model, ref_provider, ref_train_data, ref_prediction, ref_prediction_res, ref_scores = ref_datainit.getData()\n",
    "tar_model, tar_provider, tar_train_data, tar_prediction, tar_prediction_res, tar_scores = tar_datainit.getData()\n",
    "\n",
    "\n",
    "from alignment.ReferenceGenerator import ReferenceGenerator\n",
    "gen = ReferenceGenerator(ref_provider=ref_provider, tar_provider=tar_provider,REF_EPOCH=REF_EPOCH,TAR_EPOCH=TAR_EPOCH,ref_model=ref_model,tar_model=tar_model,DEVICE=DEVICE)\n",
    "\n",
    "absolute_alignment_indicates,predict_label_diff_indicates,predict_confidence_Diff_indicates,high_distance_indicates = gen.subsetClassify(18,0.8,0.3,0.05)\n",
    "\n",
    "\n",
    "from representationTrans.trans_visualizer_border import visualizer\n",
    "from singleVis.SingleVisualizationModel import VisModel\n",
    "from singleVis.projector import TimeVisProjector\n",
    "model = VisModel(ENCODER_DIMS, DECODER_DIMS)\n",
    "\n",
    "I = np.eye(512)\n",
    "projector = TimeVisProjector(vis_model=model, content_path=REF_PATH, vis_model_name=VIS_MODEL_NAME, device=\"cpu\")\n",
    "vis = visualizer(ref_provider, I,I, np.dot(ref_provider.train_representation(TAR_EPOCH),I), projector, 200,[0,1],'tab10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "def nmf(X, k,device,max_iter=100):\n",
    "    \"\"\"\n",
    "    Non-negative matrix factorization using PyTorch.\n",
    "    :param X: input data matrix of shape (n_samples, n_features)\n",
    "    :param k: number of latent factors\n",
    "    :param max_iter: maximum number of iterations\n",
    "    :return: factors W and H\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    W = torch.randn(n_samples, k, dtype=torch.float32, device=device)\n",
    "    H = torch.randn(k, n_features, dtype=torch.float32, device=device)\n",
    "    for i in range(max_iter):\n",
    "        H *= torch.mm(W.transpose(0, 1), X_tensor) / torch.mm(torch.mm(W.transpose(0, 1), W), H)\n",
    "        W *= torch.mm(X_tensor, H.transpose(0, 1)) / torch.mm(W, torch.mm(H, H.transpose(0, 1)))\n",
    "    return W.cpu().numpy(), H.cpu().numpy()\n",
    "def procrustes(X, Y):\n",
    "    \"\"\"\n",
    "    Procrustes analysis for aligning two datasets.\n",
    "    :param X: input data matrix of shape (n_samples, n_features)\n",
    "    :param Y: input data matrix of shape (n_samples, n_features)\n",
    "    :return: aligned data matrix Y\n",
    "    \"\"\"\n",
    "    # Center the data matrices\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    Y_centered = Y - np.mean(Y, axis=0)\n",
    "    # Perform Procrustes analysis\n",
    "    mtx1, mtx2, disparity = orthogonal_procrustes(X_centered, Y_centered)\n",
    "    Y_aligned = np.dot(Y_centered, mtx2)\n",
    "    return Y_aligned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NMF(nn.Module):\n",
    "    def __init__(self, n, m, r):\n",
    "        super(NMF, self).__init__()\n",
    "        self.W = nn.Parameter(torch.rand(n, r, requires_grad=True))\n",
    "        self.H = nn.Parameter(torch.rand(r, m, requires_grad=True))\n",
    "\n",
    "    def forward(self):\n",
    "        return torch.matmul(self.W, self.H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_nmf(V, r, num_epochs, learning_rate):\n",
    "    n, m = V.shape\n",
    "    nmf_model = NMF(n, m, r)\n",
    "    optimizer = optim.Adam(nmf_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        V_hat = nmf_model()\n",
    "        loss = torch.sum((V - V_hat)**2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"epoch:\",epoch,\"loss\",loss)\n",
    "    \n",
    "    return nmf_model.W, nmf_model.H\n",
    "def procrustes_analysis(X, Y):\n",
    "    X_mean = torch.mean(X, dim=0)\n",
    "    Y_mean = torch.mean(Y, dim=0)\n",
    "    \n",
    "    X_centered = X - X_mean\n",
    "    Y_centered = Y - Y_mean\n",
    "    \n",
    "    U, S, V = torch.svd(torch.matmul(X_centered.T, Y_centered))\n",
    "    R = torch.matmul(U, V.T)\n",
    "    s = torch.sum(S) / torch.sum(X_centered**2)\n",
    "\n",
    "    return R, s, X_mean, Y_mean\n",
    "\n",
    "def align_procrustes(X, Y, R, s, X_mean, Y_mean):\n",
    "    return s * torch.matmul(X - X_mean, R) + Y_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss tensor(1.0957e+08, grad_fn=<SumBackward0>)\n",
      "epoch: 1 loss tensor(42266376., grad_fn=<SumBackward0>)\n",
      "epoch: 2 loss tensor(19414678., grad_fn=<SumBackward0>)\n",
      "epoch: 3 loss tensor(15568738., grad_fn=<SumBackward0>)\n",
      "epoch: 4 loss tensor(17366386., grad_fn=<SumBackward0>)\n",
      "epoch: 5 loss tensor(19889836., grad_fn=<SumBackward0>)\n",
      "epoch: 6 loss tensor(21846224., grad_fn=<SumBackward0>)\n",
      "epoch: 7 loss tensor(23113804., grad_fn=<SumBackward0>)\n",
      "epoch: 8 loss tensor(23850860., grad_fn=<SumBackward0>)\n",
      "epoch: 9 loss tensor(24231314., grad_fn=<SumBackward0>)\n",
      "epoch: 10 loss tensor(24385930., grad_fn=<SumBackward0>)\n",
      "epoch: 11 loss tensor(24401176., grad_fn=<SumBackward0>)\n",
      "epoch: 12 loss tensor(24329976., grad_fn=<SumBackward0>)\n",
      "epoch: 13 loss tensor(24201868., grad_fn=<SumBackward0>)\n",
      "epoch: 14 loss tensor(24030270., grad_fn=<SumBackward0>)\n",
      "epoch: 15 loss tensor(23817208., grad_fn=<SumBackward0>)\n",
      "epoch: 16 loss tensor(23556222., grad_fn=<SumBackward0>)\n",
      "epoch: 17 loss tensor(23234172., grad_fn=<SumBackward0>)\n",
      "epoch: 18 loss tensor(22832402., grad_fn=<SumBackward0>)\n",
      "epoch: 19 loss tensor(22327670., grad_fn=<SumBackward0>)\n",
      "epoch: 20 loss tensor(21693400., grad_fn=<SumBackward0>)\n",
      "epoch: 21 loss tensor(20901926., grad_fn=<SumBackward0>)\n",
      "epoch: 22 loss tensor(19928954., grad_fn=<SumBackward0>)\n",
      "epoch: 23 loss tensor(18761526., grad_fn=<SumBackward0>)\n",
      "epoch: 24 loss tensor(17410694., grad_fn=<SumBackward0>)\n",
      "epoch: 25 loss tensor(15928527., grad_fn=<SumBackward0>)\n",
      "epoch: 26 loss tensor(14425204., grad_fn=<SumBackward0>)\n",
      "epoch: 27 loss tensor(13075322., grad_fn=<SumBackward0>)\n",
      "epoch: 28 loss tensor(12094741., grad_fn=<SumBackward0>)\n",
      "epoch: 29 loss tensor(11668306., grad_fn=<SumBackward0>)\n",
      "epoch: 30 loss tensor(11831684., grad_fn=<SumBackward0>)\n",
      "epoch: 31 loss tensor(12373361., grad_fn=<SumBackward0>)\n",
      "epoch: 32 loss tensor(12881223., grad_fn=<SumBackward0>)\n",
      "epoch: 33 loss tensor(12973515., grad_fn=<SumBackward0>)\n",
      "epoch: 34 loss tensor(12536874., grad_fn=<SumBackward0>)\n",
      "epoch: 35 loss tensor(11756834., grad_fn=<SumBackward0>)\n",
      "epoch: 36 loss tensor(10947274., grad_fn=<SumBackward0>)\n",
      "epoch: 37 loss tensor(10352594., grad_fn=<SumBackward0>)\n",
      "epoch: 38 loss tensor(10056077., grad_fn=<SumBackward0>)\n",
      "epoch: 39 loss tensor(10002803., grad_fn=<SumBackward0>)\n",
      "epoch: 40 loss tensor(10072941., grad_fn=<SumBackward0>)\n",
      "epoch: 41 loss tensor(10146054., grad_fn=<SumBackward0>)\n",
      "epoch: 42 loss tensor(10134588., grad_fn=<SumBackward0>)\n",
      "epoch: 43 loss tensor(9992089., grad_fn=<SumBackward0>)\n",
      "epoch: 44 loss tensor(9709005., grad_fn=<SumBackward0>)\n",
      "epoch: 45 loss tensor(9305044., grad_fn=<SumBackward0>)\n",
      "epoch: 46 loss tensor(8821288., grad_fn=<SumBackward0>)\n",
      "epoch: 47 loss tensor(8311586.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 48 loss tensor(7831413., grad_fn=<SumBackward0>)\n",
      "epoch: 49 loss tensor(7423782.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 50 loss tensor(7105314.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 51 loss tensor(6859464., grad_fn=<SumBackward0>)\n",
      "epoch: 52 loss tensor(6643924., grad_fn=<SumBackward0>)\n",
      "epoch: 53 loss tensor(6412069., grad_fn=<SumBackward0>)\n",
      "epoch: 54 loss tensor(6137366.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 55 loss tensor(5825380., grad_fn=<SumBackward0>)\n",
      "epoch: 56 loss tensor(5506399., grad_fn=<SumBackward0>)\n",
      "epoch: 57 loss tensor(5215707., grad_fn=<SumBackward0>)\n",
      "epoch: 58 loss tensor(4975261.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 59 loss tensor(4786285., grad_fn=<SumBackward0>)\n",
      "epoch: 60 loss tensor(4633194., grad_fn=<SumBackward0>)\n",
      "epoch: 61 loss tensor(4493399., grad_fn=<SumBackward0>)\n",
      "epoch: 62 loss tensor(4346993., grad_fn=<SumBackward0>)\n",
      "epoch: 63 loss tensor(4182943., grad_fn=<SumBackward0>)\n",
      "epoch: 64 loss tensor(4001006.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 65 loss tensor(3810052.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 66 loss tensor(3623938.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 67 loss tensor(3456278.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 68 loss tensor(3315672.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 69 loss tensor(3203086.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 70 loss tensor(3112486.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 71 loss tensor(3034506., grad_fn=<SumBackward0>)\n",
      "epoch: 72 loss tensor(2961189.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 73 loss tensor(2889201.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 74 loss tensor(2819872.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 75 loss tensor(2756546.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 76 loss tensor(2701247.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 77 loss tensor(2652702.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 78 loss tensor(2606559.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 79 loss tensor(2557270.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 80 loss tensor(2500438., grad_fn=<SumBackward0>)\n",
      "epoch: 81 loss tensor(2434522.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 82 loss tensor(2361345.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 83 loss tensor(2285373.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 84 loss tensor(2212205.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 85 loss tensor(2146887., grad_fn=<SumBackward0>)\n",
      "epoch: 86 loss tensor(2092660.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 87 loss tensor(2050539.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 88 loss tensor(2019707.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 89 loss tensor(1998385., grad_fn=<SumBackward0>)\n",
      "epoch: 90 loss tensor(1984612., grad_fn=<SumBackward0>)\n",
      "epoch: 91 loss tensor(1976610., grad_fn=<SumBackward0>)\n",
      "epoch: 92 loss tensor(1972695.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 93 loss tensor(1971044.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 94 loss tensor(1969638.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 95 loss tensor(1966536.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 96 loss tensor(1960337.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 97 loss tensor(1950590.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 98 loss tensor(1937916.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 99 loss tensor(1923784.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 100 loss tensor(1910016.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 101 loss tensor(1898220., grad_fn=<SumBackward0>)\n",
      "epoch: 102 loss tensor(1889367.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 103 loss tensor(1883639.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 104 loss tensor(1880560., grad_fn=<SumBackward0>)\n",
      "epoch: 105 loss tensor(1879322.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 106 loss tensor(1879122.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 107 loss tensor(1879373.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 108 loss tensor(1879738.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 109 loss tensor(1880031.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 110 loss tensor(1880097.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 111 loss tensor(1879751.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 112 loss tensor(1878811.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 113 loss tensor(1877173.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 114 loss tensor(1874892.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 115 loss tensor(1872185., grad_fn=<SumBackward0>)\n",
      "epoch: 116 loss tensor(1869373., grad_fn=<SumBackward0>)\n",
      "epoch: 117 loss tensor(1866777.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 118 loss tensor(1864629.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 119 loss tensor(1863015.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 120 loss tensor(1861900., grad_fn=<SumBackward0>)\n",
      "epoch: 121 loss tensor(1861174.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 122 loss tensor(1860720.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 123 loss tensor(1860445.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 124 loss tensor(1860286., grad_fn=<SumBackward0>)\n",
      "epoch: 125 loss tensor(1860188.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 126 loss tensor(1860097.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 127 loss tensor(1859948.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 128 loss tensor(1859694.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 129 loss tensor(1859318.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 130 loss tensor(1858847., grad_fn=<SumBackward0>)\n",
      "epoch: 131 loss tensor(1858339.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 132 loss tensor(1857866.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 133 loss tensor(1857482.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 134 loss tensor(1857208.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 135 loss tensor(1857036.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 136 loss tensor(1856938.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 137 loss tensor(1856884.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 138 loss tensor(1856851.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 139 loss tensor(1856829.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 140 loss tensor(1856811.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 141 loss tensor(1856790.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 142 loss tensor(1856757.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 143 loss tensor(1856702.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 144 loss tensor(1856622.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 145 loss tensor(1856522.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 146 loss tensor(1856414.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 147 loss tensor(1856312.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 148 loss tensor(1856228.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 149 loss tensor(1856166.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 150 loss tensor(1856123.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 151 loss tensor(1856094.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 152 loss tensor(1856073.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 153 loss tensor(1856057.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 154 loss tensor(1856044.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 155 loss tensor(1856035.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 156 loss tensor(1856026.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 157 loss tensor(1856016., grad_fn=<SumBackward0>)\n",
      "epoch: 158 loss tensor(1856002.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 159 loss tensor(1855984.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 160 loss tensor(1855963.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 161 loss tensor(1855943.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 162 loss tensor(1855926., grad_fn=<SumBackward0>)\n",
      "epoch: 163 loss tensor(1855912.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 164 loss tensor(1855903.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 165 loss tensor(1855897.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 166 loss tensor(1855893., grad_fn=<SumBackward0>)\n",
      "epoch: 167 loss tensor(1855889.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 168 loss tensor(1855885.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 169 loss tensor(1855883.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 170 loss tensor(1855881.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 171 loss tensor(1855878.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 172 loss tensor(1855875.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 173 loss tensor(1855872., grad_fn=<SumBackward0>)\n",
      "epoch: 174 loss tensor(1855867.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 175 loss tensor(1855863.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 176 loss tensor(1855859., grad_fn=<SumBackward0>)\n",
      "epoch: 177 loss tensor(1855855.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 178 loss tensor(1855853.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 179 loss tensor(1855851.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 180 loss tensor(1855850.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 181 loss tensor(1855849.1250, grad_fn=<SumBackward0>)\n",
      "epoch: 182 loss tensor(1855848., grad_fn=<SumBackward0>)\n",
      "epoch: 183 loss tensor(1855847.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 184 loss tensor(1855846.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 185 loss tensor(1855845.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 186 loss tensor(1855845., grad_fn=<SumBackward0>)\n",
      "epoch: 187 loss tensor(1855844.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 188 loss tensor(1855843.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 189 loss tensor(1855842.3750, grad_fn=<SumBackward0>)\n",
      "epoch: 190 loss tensor(1855841.6250, grad_fn=<SumBackward0>)\n",
      "epoch: 191 loss tensor(1855840.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 192 loss tensor(1855840.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 193 loss tensor(1855840., grad_fn=<SumBackward0>)\n",
      "epoch: 194 loss tensor(1855839.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 195 loss tensor(1855839.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 196 loss tensor(1855839.2500, grad_fn=<SumBackward0>)\n",
      "epoch: 197 loss tensor(1855838.8750, grad_fn=<SumBackward0>)\n",
      "epoch: 198 loss tensor(1855838.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 199 loss tensor(1855838.7500, grad_fn=<SumBackward0>)\n",
      "epoch: 0 loss tensor(1.0194e+08, grad_fn=<SumBackward0>)\n",
      "epoch: 1 loss tensor(39538148., grad_fn=<SumBackward0>)\n",
      "epoch: 2 loss tensor(20560620., grad_fn=<SumBackward0>)\n",
      "epoch: 3 loss tensor(19047836., grad_fn=<SumBackward0>)\n",
      "epoch: 4 loss tensor(22036404., grad_fn=<SumBackward0>)\n",
      "epoch: 5 loss tensor(25148192., grad_fn=<SumBackward0>)\n",
      "epoch: 6 loss tensor(27416750., grad_fn=<SumBackward0>)\n",
      "epoch: 7 loss tensor(28869520., grad_fn=<SumBackward0>)\n",
      "epoch: 8 loss tensor(29721876., grad_fn=<SumBackward0>)\n",
      "epoch: 9 loss tensor(30163686., grad_fn=<SumBackward0>)\n",
      "epoch: 10 loss tensor(30324068., grad_fn=<SumBackward0>)\n",
      "epoch: 11 loss tensor(30280180., grad_fn=<SumBackward0>)\n",
      "epoch: 12 loss tensor(30071954., grad_fn=<SumBackward0>)\n",
      "epoch: 13 loss tensor(29714122., grad_fn=<SumBackward0>)\n",
      "epoch: 14 loss tensor(29204934., grad_fn=<SumBackward0>)\n",
      "epoch: 15 loss tensor(28532952., grad_fn=<SumBackward0>)\n",
      "epoch: 16 loss tensor(27683320., grad_fn=<SumBackward0>)\n",
      "epoch: 17 loss tensor(26644692., grad_fn=<SumBackward0>)\n",
      "epoch: 18 loss tensor(25417220., grad_fn=<SumBackward0>)\n",
      "epoch: 19 loss tensor(24021028., grad_fn=<SumBackward0>)\n",
      "epoch: 20 loss tensor(22503472., grad_fn=<SumBackward0>)\n",
      "epoch: 21 loss tensor(20941910., grad_fn=<SumBackward0>)\n",
      "epoch: 22 loss tensor(19437960., grad_fn=<SumBackward0>)\n",
      "epoch: 23 loss tensor(18100438., grad_fn=<SumBackward0>)\n",
      "epoch: 24 loss tensor(17018534., grad_fn=<SumBackward0>)\n",
      "epoch: 25 loss tensor(16233864., grad_fn=<SumBackward0>)\n",
      "epoch: 26 loss tensor(15726014., grad_fn=<SumBackward0>)\n",
      "epoch: 27 loss tensor(15423564., grad_fn=<SumBackward0>)\n",
      "epoch: 28 loss tensor(15237221., grad_fn=<SumBackward0>)\n",
      "epoch: 29 loss tensor(15095579., grad_fn=<SumBackward0>)\n",
      "epoch: 30 loss tensor(14963376., grad_fn=<SumBackward0>)\n",
      "epoch: 31 loss tensor(14836656., grad_fn=<SumBackward0>)\n",
      "epoch: 32 loss tensor(14724574., grad_fn=<SumBackward0>)\n",
      "epoch: 33 loss tensor(14632116., grad_fn=<SumBackward0>)\n",
      "epoch: 34 loss tensor(14551865., grad_fn=<SumBackward0>)\n",
      "epoch: 35 loss tensor(14465234., grad_fn=<SumBackward0>)\n",
      "epoch: 36 loss tensor(14349158., grad_fn=<SumBackward0>)\n",
      "epoch: 37 loss tensor(14183972., grad_fn=<SumBackward0>)\n",
      "epoch: 38 loss tensor(13959529., grad_fn=<SumBackward0>)\n",
      "epoch: 39 loss tensor(13678325., grad_fn=<SumBackward0>)\n",
      "epoch: 40 loss tensor(13355115., grad_fn=<SumBackward0>)\n",
      "epoch: 41 loss tensor(13012939., grad_fn=<SumBackward0>)\n",
      "epoch: 42 loss tensor(12676006., grad_fn=<SumBackward0>)\n",
      "epoch: 43 loss tensor(12361202., grad_fn=<SumBackward0>)\n",
      "epoch: 44 loss tensor(12071556., grad_fn=<SumBackward0>)\n",
      "epoch: 45 loss tensor(11795434., grad_fn=<SumBackward0>)\n",
      "epoch: 46 loss tensor(11513016., grad_fn=<SumBackward0>)\n",
      "epoch: 47 loss tensor(11207246., grad_fn=<SumBackward0>)\n",
      "epoch: 48 loss tensor(10872987., grad_fn=<SumBackward0>)\n",
      "epoch: 49 loss tensor(10518973., grad_fn=<SumBackward0>)\n",
      "epoch: 50 loss tensor(10162110., grad_fn=<SumBackward0>)\n",
      "epoch: 51 loss tensor(9818658., grad_fn=<SumBackward0>)\n",
      "epoch: 52 loss tensor(9497886., grad_fn=<SumBackward0>)\n",
      "epoch: 53 loss tensor(9200995., grad_fn=<SumBackward0>)\n",
      "epoch: 54 loss tensor(8924235., grad_fn=<SumBackward0>)\n",
      "epoch: 55 loss tensor(8663341., grad_fn=<SumBackward0>)\n",
      "epoch: 56 loss tensor(8416628., grad_fn=<SumBackward0>)\n",
      "epoch: 57 loss tensor(8185610., grad_fn=<SumBackward0>)\n",
      "epoch: 58 loss tensor(7973435., grad_fn=<SumBackward0>)\n",
      "epoch: 59 loss tensor(7782337., grad_fn=<SumBackward0>)\n",
      "epoch: 60 loss tensor(7611598.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 61 loss tensor(7457145., grad_fn=<SumBackward0>)\n",
      "epoch: 62 loss tensor(7312983., grad_fn=<SumBackward0>)\n",
      "epoch: 63 loss tensor(7173658.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 64 loss tensor(7036348.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 65 loss tensor(6901441.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 66 loss tensor(6771439., grad_fn=<SumBackward0>)\n",
      "epoch: 67 loss tensor(6648962.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 68 loss tensor(6535077., grad_fn=<SumBackward0>)\n",
      "epoch: 69 loss tensor(6428717.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 70 loss tensor(6327278., grad_fn=<SumBackward0>)\n",
      "epoch: 71 loss tensor(6227832.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 72 loss tensor(6128236.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 73 loss tensor(6027651., grad_fn=<SumBackward0>)\n",
      "epoch: 74 loss tensor(5926386., grad_fn=<SumBackward0>)\n",
      "epoch: 75 loss tensor(5825331., grad_fn=<SumBackward0>)\n",
      "epoch: 76 loss tensor(5725401.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 77 loss tensor(5627281.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 78 loss tensor(5531547., grad_fn=<SumBackward0>)\n",
      "epoch: 79 loss tensor(5438940., grad_fn=<SumBackward0>)\n",
      "epoch: 80 loss tensor(5350523.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 81 loss tensor(5267504., grad_fn=<SumBackward0>)\n",
      "epoch: 82 loss tensor(5190812., grad_fn=<SumBackward0>)\n",
      "epoch: 83 loss tensor(5120670.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 84 loss tensor(5056437., grad_fn=<SumBackward0>)\n",
      "epoch: 85 loss tensor(4996821., grad_fn=<SumBackward0>)\n",
      "epoch: 86 loss tensor(4940403., grad_fn=<SumBackward0>)\n",
      "epoch: 87 loss tensor(4886198.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 88 loss tensor(4834041.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 89 loss tensor(4784623., grad_fn=<SumBackward0>)\n",
      "epoch: 90 loss tensor(4739214., grad_fn=<SumBackward0>)\n",
      "epoch: 91 loss tensor(4699201., grad_fn=<SumBackward0>)\n",
      "epoch: 92 loss tensor(4665618.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 93 loss tensor(4638844., grad_fn=<SumBackward0>)\n",
      "epoch: 94 loss tensor(4618496.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 95 loss tensor(4603540., grad_fn=<SumBackward0>)\n",
      "epoch: 96 loss tensor(4592525.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 97 loss tensor(4583886., grad_fn=<SumBackward0>)\n",
      "epoch: 98 loss tensor(4576224., grad_fn=<SumBackward0>)\n",
      "epoch: 99 loss tensor(4568536., grad_fn=<SumBackward0>)\n",
      "epoch: 100 loss tensor(4560311.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 101 loss tensor(4551525., grad_fn=<SumBackward0>)\n",
      "epoch: 102 loss tensor(4542506., grad_fn=<SumBackward0>)\n",
      "epoch: 103 loss tensor(4533754., grad_fn=<SumBackward0>)\n",
      "epoch: 104 loss tensor(4525759., grad_fn=<SumBackward0>)\n",
      "epoch: 105 loss tensor(4518867., grad_fn=<SumBackward0>)\n",
      "epoch: 106 loss tensor(4513233.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 107 loss tensor(4508840.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 108 loss tensor(4505551., grad_fn=<SumBackward0>)\n",
      "epoch: 109 loss tensor(4503173., grad_fn=<SumBackward0>)\n",
      "epoch: 110 loss tensor(4501496., grad_fn=<SumBackward0>)\n",
      "epoch: 111 loss tensor(4500306., grad_fn=<SumBackward0>)\n",
      "epoch: 112 loss tensor(4499395., grad_fn=<SumBackward0>)\n",
      "epoch: 113 loss tensor(4498566.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 114 loss tensor(4497655., grad_fn=<SumBackward0>)\n",
      "epoch: 115 loss tensor(4496542., grad_fn=<SumBackward0>)\n",
      "epoch: 116 loss tensor(4495174.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 117 loss tensor(4493557., grad_fn=<SumBackward0>)\n",
      "epoch: 118 loss tensor(4491745., grad_fn=<SumBackward0>)\n",
      "epoch: 119 loss tensor(4489824.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 120 loss tensor(4487893., grad_fn=<SumBackward0>)\n",
      "epoch: 121 loss tensor(4486049., grad_fn=<SumBackward0>)\n",
      "epoch: 122 loss tensor(4484377., grad_fn=<SumBackward0>)\n",
      "epoch: 123 loss tensor(4482941., grad_fn=<SumBackward0>)\n",
      "epoch: 124 loss tensor(4481774., grad_fn=<SumBackward0>)\n",
      "epoch: 125 loss tensor(4480875.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 126 loss tensor(4480211., grad_fn=<SumBackward0>)\n",
      "epoch: 127 loss tensor(4479725., grad_fn=<SumBackward0>)\n",
      "epoch: 128 loss tensor(4479353.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 129 loss tensor(4479037., grad_fn=<SumBackward0>)\n",
      "epoch: 130 loss tensor(4478733., grad_fn=<SumBackward0>)\n",
      "epoch: 131 loss tensor(4478415.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 132 loss tensor(4478077., grad_fn=<SumBackward0>)\n",
      "epoch: 133 loss tensor(4477721.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 134 loss tensor(4477362.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 135 loss tensor(4477015., grad_fn=<SumBackward0>)\n",
      "epoch: 136 loss tensor(4476696.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 137 loss tensor(4476420.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 138 loss tensor(4476194.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 139 loss tensor(4476019.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 140 loss tensor(4475890., grad_fn=<SumBackward0>)\n",
      "epoch: 141 loss tensor(4475797., grad_fn=<SumBackward0>)\n",
      "epoch: 142 loss tensor(4475729., grad_fn=<SumBackward0>)\n",
      "epoch: 143 loss tensor(4475675., grad_fn=<SumBackward0>)\n",
      "epoch: 144 loss tensor(4475626.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 145 loss tensor(4475576.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 146 loss tensor(4475520., grad_fn=<SumBackward0>)\n",
      "epoch: 147 loss tensor(4475455.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 148 loss tensor(4475384., grad_fn=<SumBackward0>)\n",
      "epoch: 149 loss tensor(4475308., grad_fn=<SumBackward0>)\n",
      "epoch: 150 loss tensor(4475232., grad_fn=<SumBackward0>)\n",
      "epoch: 151 loss tensor(4475160., grad_fn=<SumBackward0>)\n",
      "epoch: 152 loss tensor(4475095.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 153 loss tensor(4475041., grad_fn=<SumBackward0>)\n",
      "epoch: 154 loss tensor(4474996.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 155 loss tensor(4474961.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 156 loss tensor(4474935., grad_fn=<SumBackward0>)\n",
      "epoch: 157 loss tensor(4474914.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 158 loss tensor(4474897.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 159 loss tensor(4474882., grad_fn=<SumBackward0>)\n",
      "epoch: 160 loss tensor(4474867., grad_fn=<SumBackward0>)\n",
      "epoch: 161 loss tensor(4474851., grad_fn=<SumBackward0>)\n",
      "epoch: 162 loss tensor(4474834., grad_fn=<SumBackward0>)\n",
      "epoch: 163 loss tensor(4474816.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 164 loss tensor(4474799., grad_fn=<SumBackward0>)\n",
      "epoch: 165 loss tensor(4474783.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 166 loss tensor(4474769.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 167 loss tensor(4474758., grad_fn=<SumBackward0>)\n",
      "epoch: 168 loss tensor(4474748.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 169 loss tensor(4474742., grad_fn=<SumBackward0>)\n",
      "epoch: 170 loss tensor(4474736.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 171 loss tensor(4474733., grad_fn=<SumBackward0>)\n",
      "epoch: 172 loss tensor(4474730., grad_fn=<SumBackward0>)\n",
      "epoch: 173 loss tensor(4474727., grad_fn=<SumBackward0>)\n",
      "epoch: 174 loss tensor(4474723., grad_fn=<SumBackward0>)\n",
      "epoch: 175 loss tensor(4474720.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 176 loss tensor(4474716.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 177 loss tensor(4474712., grad_fn=<SumBackward0>)\n",
      "epoch: 178 loss tensor(4474708.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 179 loss tensor(4474705., grad_fn=<SumBackward0>)\n",
      "epoch: 180 loss tensor(4474701.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 181 loss tensor(4474698.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 182 loss tensor(4474696.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 183 loss tensor(4474695., grad_fn=<SumBackward0>)\n",
      "epoch: 184 loss tensor(4474694., grad_fn=<SumBackward0>)\n",
      "epoch: 185 loss tensor(4474692.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 186 loss tensor(4474692., grad_fn=<SumBackward0>)\n",
      "epoch: 187 loss tensor(4474691., grad_fn=<SumBackward0>)\n",
      "epoch: 188 loss tensor(4474690., grad_fn=<SumBackward0>)\n",
      "epoch: 189 loss tensor(4474688.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 190 loss tensor(4474688., grad_fn=<SumBackward0>)\n",
      "epoch: 191 loss tensor(4474687.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 192 loss tensor(4474686., grad_fn=<SumBackward0>)\n",
      "epoch: 193 loss tensor(4474685.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 194 loss tensor(4474684.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 195 loss tensor(4474684., grad_fn=<SumBackward0>)\n",
      "epoch: 196 loss tensor(4474683.5000, grad_fn=<SumBackward0>)\n",
      "epoch: 197 loss tensor(4474683., grad_fn=<SumBackward0>)\n",
      "epoch: 198 loss tensor(4474683., grad_fn=<SumBackward0>)\n",
      "epoch: 199 loss tensor(4474682.5000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor(tar_train_data)\n",
    "Y = torch.Tensor(ref_train_data)\n",
    "# 计算X和Y的均值和标准差\n",
    "# Perform NMF on X and Y\n",
    "W_X, H_X = perform_nmf(X, 10, 200, 0.1)\n",
    "W_Y, H_Y = perform_nmf(Y, 10, 200, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, s, W_X_mean, W_Y_mean = procrustes_analysis(W_X, W_Y)\n",
    "W_X_aligned = align_procrustes(W_X, W_Y, R, s, W_X_mean, W_Y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0192e-01,  2.7818e-01,  8.3335e-02,  ..., -4.1830e-01,\n",
       "         -3.1814e-01,  2.7906e-01],\n",
       "        [-6.1115e-02, -1.3147e-01,  2.1422e-02,  ...,  5.1416e-02,\n",
       "         -2.2498e-01,  3.7401e-02],\n",
       "        [-2.5321e-01,  1.5755e-01,  1.0832e-01,  ...,  2.4374e-01,\n",
       "          2.4707e-01,  3.2663e-01],\n",
       "        ...,\n",
       "        [ 1.4403e-01, -3.5164e-01,  3.0339e-05,  ...,  2.9838e-01,\n",
       "          3.6142e-01,  2.8500e-02],\n",
       "        [ 7.9456e-02, -1.6207e-02, -1.8104e-01,  ..., -3.4400e-01,\n",
       "          2.2521e-01, -4.1308e-02],\n",
       "        [ 1.5862e-01, -2.3774e-01, -1.6660e-01,  ..., -1.6032e-01,\n",
       "          3.1075e-01, -2.8092e-01]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distances = pairwise_distances(W_X_aligned, W_Y)\n",
    "W_X_aligned - W_Y\n",
    "# degrees = np.sqrt(np.sum((W_X_aligned - W_Y)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8112, 2.1760, 1.6504,  ..., 1.8227, 2.1220, 1.7615],\n",
       "        [2.4204, 0.5849, 0.6242,  ..., 0.7195, 2.4144, 1.4973],\n",
       "        [2.2690, 1.0002, 0.6661,  ..., 0.8080, 2.5014, 1.6200],\n",
       "        ...,\n",
       "        [1.9942, 0.9942, 0.5425,  ..., 0.6734, 2.0870, 1.2926],\n",
       "        [2.3369, 2.3433, 2.3271,  ..., 2.3177, 0.5497, 1.3969],\n",
       "        [2.0177, 1.7465, 1.4984,  ..., 1.6875, 1.4423, 0.6278]],\n",
       "       grad_fn=<SqrtBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_similar_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "X = W_X_aligned.detach().numpy()\n",
    "Y = W_Y.detach().numpy()\n",
    "percentile=90\n",
    "# Perform Procrustes analysis to transform X to Y\n",
    "mtx1, mtx2, disparity = procrustes(X, Y)\n",
    "# Compute the transformation degree for each row of the matrix\n",
    "degree = np.sqrt(np.sum((mtx1 - X)**2, axis=1))\n",
    "\n",
    "# Find the subset of samples with the highest transformation degree\n",
    "threshold = np.percentile(degree, percentile)\n",
    "high_discrepancy_representation_set = np.where(degree > threshold)[0]\n",
    "\n",
    "threshold_2 = np.percentile(degree, 20)\n",
    "low_discrepancy_representation_set = np.where(degree < threshold_2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tar_train_data\n",
    "Y = ref_train_data\n",
    "percentile=90\n",
    "# Perform Procrustes analysis to transform X to Y\n",
    "mtx1, mtx2, disparity = procrustes(X, Y)\n",
    "# Compute the transformation degree for each row of the matrix\n",
    "degree = np.sqrt(np.sum((mtx1 - X)**2, axis=1))\n",
    "\n",
    "# Find the subset of samples with the highest transformation degree\n",
    "threshold = np.percentile(degree, percentile)\n",
    "threshold_2 = np.percentile(degree, 20)\n",
    "high_discrepancy_representation_set = np.where(degree > threshold)[0]\n",
    "low_discrepancy_representation_set = np.where(degree < threshold_2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.48912018, 26.55234335, 25.3769415 , ..., 18.35612092,\n",
       "       31.3011138 , 20.43379256])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 samples in high prediction MAE set\n",
      "32/97 samples of high prediction MAE are present in high_discrepancy_representation_set.\n",
      "106 samples in absolute alignmeny set\n",
      "30/106 samples of absolute_alignment_indicates are present in high_discrepancy_representation_set.\n",
      "106 samples in absolute alignmeny set\n",
      "11/106 samples of absolute_alignment_indicates are present in low_discrepancy_representation_set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"{len(high_distance_indicates)} samples in high prediction MAE set\")\n",
    "in_both_arrays = np.in1d(high_discrepancy_representation_set, high_distance_indicates)\n",
    "\n",
    "print(f\"{np.count_nonzero(in_both_arrays)}/{len(high_distance_indicates)} samples of high prediction MAE are present in high_discrepancy_representation_set.\")\n",
    "\n",
    "\n",
    "print(f\"{len(absolute_alignment_indicates)} samples in absolute alignmeny set\")\n",
    "in_both_arrays = np.in1d(high_discrepancy_representation_set, absolute_alignment_indicates)\n",
    "print(f\"{np.count_nonzero(in_both_arrays)}/{len(absolute_alignment_indicates)} samples of absolute_alignment_indicates are present in high_discrepancy_representation_set.\")\n",
    "\n",
    "\n",
    "print(f\"{len(absolute_alignment_indicates)} samples in absolute alignmeny set\")\n",
    "in_both_arrays = np.in1d(low_discrepancy_representation_set, absolute_alignment_indicates)\n",
    "print(f\"{np.count_nonzero(in_both_arrays)}/{len(absolute_alignment_indicates)} samples of absolute_alignment_indicates are present in low_discrepancy_representation_set.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdebugger",
   "language": "python",
   "name": "deepdebugger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
