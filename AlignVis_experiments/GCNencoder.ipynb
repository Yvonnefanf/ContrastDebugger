{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/miniconda3/envs/deepdebugger/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET resnet18\n",
      "Finish initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 282.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET resnet18_with_dropout\n",
      "Finish initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 3417.75it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 4504.60it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 5056.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute alignment indicates number: 106 label diff indicates number: 12 confidence diff indicates number: 16 high distance number: 97\n"
     ]
    }
   ],
   "source": [
    "####### dropout resnet18 vs without dropout\n",
    "#### \n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "\n",
    "CLEAN_PATH = \"/home/yifan/dataset/resnet18_with_dropout/pairflip/cifar10/0/\"\n",
    "REF_PATH = \"/home/yifan/dataset/clean/pairflip/cifar10/0\"\n",
    "\n",
    "\n",
    "ENCODER_DIMS=[512,256,256,256,256,2]\n",
    "DECODER_DIMS= [2,256,256,256,256,512]\n",
    "VIS_MODEL_NAME = 'vis'\n",
    "\n",
    "DEVICE='cuda:0'\n",
    "########## initulize reference data and target data\n",
    "from alignment.data_preprocess import DataInit\n",
    "REF_EPOCH = 200\n",
    "TAR_EPOCH = 200\n",
    "ref_datainit = DataInit(REF_PATH,REF_PATH,REF_EPOCH)\n",
    "tar_datainit = DataInit(CLEAN_PATH,CLEAN_PATH,TAR_EPOCH)\n",
    "ref_model, ref_provider, ref_train_data, ref_prediction, ref_prediction_res, ref_scores = ref_datainit.getData()\n",
    "tar_model, tar_provider, tar_train_data, tar_prediction, tar_prediction_res, tar_scores = tar_datainit.getData()\n",
    "\n",
    "\n",
    "from alignment.ReferenceGenerator import ReferenceGenerator\n",
    "gen = ReferenceGenerator(ref_provider=ref_provider, tar_provider=tar_provider,REF_EPOCH=REF_EPOCH,TAR_EPOCH=TAR_EPOCH,ref_model=ref_model,tar_model=tar_model,DEVICE=DEVICE)\n",
    "\n",
    "absolute_alignment_indicates,predict_label_diff_indicates,predict_confidence_Diff_indicates,high_distance_indicates = gen.subsetClassify(mes_val_for_diff=18,mes_val_for_same=0.8,conf_val_for_diff=0.3,conf_val_for_same=0.05)\n",
    "\n",
    "\n",
    "from representationTrans.trans_visualizer_border import visualizer\n",
    "from singleVis.SingleVisualizationModel import VisModel\n",
    "from singleVis.projector import TimeVisProjector\n",
    "model = VisModel(ENCODER_DIMS, DECODER_DIMS)\n",
    "\n",
    "I = np.eye(512)\n",
    "projector = TimeVisProjector(vis_model=model, content_path=REF_PATH, vis_model_name=VIS_MODEL_NAME, device=\"cpu\")\n",
    "vis = visualizer(ref_provider, I,I, np.dot(ref_provider.train_representation(TAR_EPOCH),I), projector, 200,[0,1],'tab10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AlignVis.autoencoder import GCNAutoencoder\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "X = torch.Tensor(ref_train_data)\n",
    "Y = torch.Tensor(tar_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(X, Y, output_dim, batch_size=128):\n",
    "    # Build the graph\n",
    "    Y_dist = np.sqrt(np.sum((Y[:, None, :] - Y[None, :, :])**2, axis=-1))\n",
    "    adj = np.exp(-Y_dist**2 / (2 * np.median(Y_dist)**2))\n",
    "    adj = adj / adj.sum(axis=1, keepdims=True)\n",
    "    adj = torch.tensor(adj).float()\n",
    "\n",
    "    # Normalize the input data\n",
    "    X_norm = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    Y_norm = (Y - Y.mean(axis=0)) / Y.std(axis=0)\n",
    "\n",
    "    # Initialize the model and the optimizer\n",
    "    input_dim = X_norm.shape[1]\n",
    "    hidden_dim = 32\n",
    "    autoencoder = GCNAutoencoder(input_dim, output_dim, hidden_dim)\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train the autoencoder\n",
    "    for epoch in range(100):\n",
    "        # Shuffle the input data\n",
    "        indices = torch.randperm(Y_norm.shape[0])\n",
    "        Y_norm = Y_norm[indices]\n",
    "\n",
    "        # Process the input data in batches\n",
    "        for i in range(0, Y_norm.shape[0], batch_size):\n",
    "            batch = Y_norm[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            output_data, encoded_Y = autoencoder(batch, adj)\n",
    "\n",
    "            # Compute the MGM loss\n",
    "            Y_dist_batch = Y_dist[i:i+batch_size, i:i+batch_size]\n",
    "            output_dist = np.sqrt(np.sum((encoded_Y[:, None, :] - encoded_Y[None, :, :])**2, axis=-1))\n",
    "            output_dist = torch.tensor(output_dist).float()\n",
    "            output_dist_batch = output_dist[i:i+batch_size, i:i+batch_size]\n",
    "            loss1 = torch.mean((Y_dist_batch - output_dist_batch)**2 / (Y_dist_batch**2 + 1e-8))\n",
    "\n",
    "            # Add GCN regularization term\n",
    "            loss2 = torch.norm(torch.mm(adj, encoded_Y), p=2)\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss1 + 0.01 * loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Extract the transformed data Y_trans\n",
    "    with torch.no_grad():\n",
    "        _, encoded_Y = autoencoder(Y_norm, adj)\n",
    "        Y_trans = encoded_Y.detach().numpy()\n",
    "\n",
    "    # Compute the Frobenius norm between X and Y_trans\n",
    "    fro_norm = torch.norm(X_norm - encoded_Y[:X_norm.shape[0], :], p='fro')\n",
    "\n",
    "    # Compute the CKA between Y and Y_trans\n",
    "    Y_norm_flat = Y_norm.flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "input_dim = 512\n",
    "hidden_dim = 2\n",
    "lr = 0.01\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 5120000000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_274706/3617961914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcka\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_274706/3037622554.py\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[0;34m(X, Y, output_dim)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Build the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mY_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY_dist\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:66] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 5120000000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "Y_trans, norm, cka = train_autoencoder(X, Y, input_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdebugger",
   "language": "python",
   "name": "deepdebugger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
