{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper on anomarly detection experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from singleVis.SingleVisualizationModel import SingleVisualizationModel\n",
    "from singleVis.data import NormalDataProvider\n",
    "from singleVis.eval.evaluator import Evaluator\n",
    "from singleVis.projector import Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"symmetric\"\n",
    "CONTENT_PATH =\"/home/xianglin/projects/DVI_data/noisy/{}/10/\".format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(CONTENT_PATH)\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_ID = 0\n",
    "SETTING = config[\"SETTING\"]\n",
    "CLASSES = config[\"CLASSES\"]\n",
    "DATASET = config[\"DATASET\"]\n",
    "PREPROCESS = config[\"VISUALIZATION\"][\"PREPROCESS\"]\n",
    "EPOCH_START = config[\"EPOCH_START\"]\n",
    "EPOCH_END = config[\"EPOCH_END\"]\n",
    "EPOCH_PERIOD = config[\"EPOCH_PERIOD\"]\n",
    "\n",
    "# Training parameter (subject model)\n",
    "TRAINING_PARAMETER = config[\"TRAINING\"]\n",
    "NET = TRAINING_PARAMETER[\"NET\"]\n",
    "LEN = TRAINING_PARAMETER[\"train_num\"]\n",
    "\n",
    "# Training parameter (visualization model)\n",
    "VISUALIZATION_PARAMETER = config[\"VISUALIZATION\"]\n",
    "LAMBDA = VISUALIZATION_PARAMETER[\"LAMBDA\"]\n",
    "S_LAMBDA = VISUALIZATION_PARAMETER[\"S_LAMBDA\"]\n",
    "B_N_EPOCHS = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"B_N_EPOCHS\"]\n",
    "L_BOUND = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"L_BOUND\"]\n",
    "INIT_NUM = VISUALIZATION_PARAMETER[\"INIT_NUM\"]\n",
    "ALPHA = VISUALIZATION_PARAMETER[\"ALPHA\"]\n",
    "BETA = VISUALIZATION_PARAMETER[\"BETA\"]\n",
    "MAX_HAUSDORFF = VISUALIZATION_PARAMETER[\"MAX_HAUSDORFF\"]\n",
    "HIDDEN_LAYER = VISUALIZATION_PARAMETER[\"HIDDEN_LAYER\"]\n",
    "S_N_EPOCHS = VISUALIZATION_PARAMETER[\"S_N_EPOCHS\"]\n",
    "T_N_EPOCHS = VISUALIZATION_PARAMETER[\"T_N_EPOCHS\"]\n",
    "N_NEIGHBORS = VISUALIZATION_PARAMETER[\"N_NEIGHBORS\"]\n",
    "PATIENT = VISUALIZATION_PARAMETER[\"PATIENT\"]\n",
    "MAX_EPOCH = VISUALIZATION_PARAMETER[\"MAX_EPOCH\"]\n",
    "SEGMENTS = VISUALIZATION_PARAMETER[\"SEGMENTS\"]\n",
    "RESUME_SEG = VISUALIZATION_PARAMETER[\"RESUME_SEG\"]\n",
    "# define hyperparameters\n",
    "DEVICE = torch.device(\"cuda:{}\".format(GPU_ID) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "content_path = CONTENT_PATH\n",
    "sys.path.append(content_path)\n",
    "\n",
    "import Model.model as subject_model\n",
    "# net = resnet18()\n",
    "net = eval(\"subject_model.{}()\".format(NET))\n",
    "classes = (\"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish initialization...\n"
     ]
    }
   ],
   "source": [
    "data_provider = NormalDataProvider(CONTENT_PATH, net, EPOCH_START, EPOCH_END, EPOCH_PERIOD, split=-1, device=DEVICE, classes=CLASSES,verbose=1)\n",
    "# if PREPROCESS:\n",
    "#     data_provider.initialize(LEN//10, l_bound=L_BOUND)\n",
    "\n",
    "model = SingleVisualizationModel(input_dims=512, output_dims=2, units=256, hidden_layer=HIDDEN_LAYER)\n",
    "projector = Projector(vis_model=model, content_path=CONTENT_PATH, segments=SEGMENTS, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from singleVis.visualizer import visualizer\n",
    "vis = visualizer(data_provider, projector, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/xianglin/projects/DVI_data/noisy/{}/10/clean_label.json\".format(dataset)\n",
    "with open(path, \"r\") as f:\n",
    "    clean_label = json.load(f)\n",
    "path = \"/home/xianglin/projects/DVI_data/noisy/{}/10/noisy_label.json\".format(dataset)\n",
    "with open(path, \"r\") as f:\n",
    "    noisy_label = json.load(f)\n",
    "clean_label = np.array(clean_label)\n",
    "noisy_label = np.array(noisy_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = data_provider.train_labels(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros((200, 50000, 512))\n",
    "for i in range(1, 201, 1):\n",
    "    samples[i-1] = data_provider.train_representation(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load the visualization model for range (1,32)...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Successfully load the visualization model for range (32,104)...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Successfully load the visualization model for range (104,162)...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Successfully load the visualization model for range (162,200)...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n",
      "Same range as current visualization model...\n"
     ]
    }
   ],
   "source": [
    "embeddings_2d = np.zeros((200, 50000, 2))\n",
    "for e in range(1, 201, 1):\n",
    "    embeddings_2d[e-1] = projector.batch_project(e, samples[e-1])\n",
    "embeddings_2d = np.transpose(embeddings_2d, [1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/xianglin/projects/DVI_data/noisy/{}/10/embedding.npy\".format(dataset)\n",
    "np.save(path, embeddings_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_2d[:,80:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 119, 2), (50000, 118, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = embeddings[:,1:,:]-embeddings[:,:-1,:]\n",
    "a = v[:,1:,:]-v[:,:-1,:]\n",
    "v.shape, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birch(n_clusters=30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import Birch\n",
    "brc = Birch(n_clusters=30)\n",
    "brc.fit(a.reshape(50000, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 30\n",
    "gt = np.argwhere(noisy_label!=clean_label).squeeze()\n",
    "rates = np.zeros(c)\n",
    "wrong = np.zeros(c)\n",
    "sel = np.zeros(c)\n",
    "for suspect in range(c):\n",
    "    selected_idxs = np.argwhere(brc.labels_==suspect).squeeze()\n",
    "    wrong[suspect] = len(np.intersect1d(selected_idxs, gt))\n",
    "    sel[suspect] = len(selected_idxs)\n",
    "    rates[suspect] = len(np.intersect1d(selected_idxs, gt))/len(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.406 \t 183 / 451\n",
      "0.327 \t 205 / 627\n",
      "0.297 \t 251 / 846\n",
      "0.241 \t 127 / 527\n",
      "0.205 \t 76 / 370\n",
      "0.192 \t 74 / 386\n",
      "0.184 \t 190 / 1031\n",
      "0.168 \t 51 / 303\n",
      "0.16 \t 56 / 349\n",
      "0.155 \t 50 / 323\n",
      "0.149 \t 48 / 323\n",
      "0.12 \t 184 / 1527\n",
      "0.111 \t 85 / 769\n",
      "0.105 \t 67 / 637\n",
      "0.101 \t 131 / 1296\n",
      "0.099 \t 60 / 608\n",
      "0.092 \t 54 / 587\n",
      "0.086 \t 88 / 1022\n",
      "0.085 \t 54 / 633\n",
      "0.083 \t 1576 / 18896\n",
      "0.082 \t 65 / 794\n",
      "0.082 \t 28 / 343\n",
      "0.081 \t 47 / 579\n",
      "0.08 \t 34 / 423\n",
      "0.077 \t 184 / 2387\n",
      "0.074 \t 77 / 1037\n",
      "0.068 \t 736 / 10828\n",
      "0.062 \t 25 / 400\n",
      "0.05 \t 50 / 995\n",
      "0.05 \t 35 / 703\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(rates)\n",
    "for r in range(c-1,-1,-1):\n",
    "    print(round(rates[ranking[r]], 3),\"\\t\", int(wrong[ranking[r]]),\"/\", int(sel[ranking[r]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_idxs = np.argwhere(clean_label!=noisy_label).squeeze()\n",
    "selected = np.random.choice(noise_idxs,size=300,replace=False)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(a.reshape(50000, -1))\n",
    "_, idxs = nbrs.kneighbors(a.reshape(50000, -1)[selected])\n",
    "selected_idxs = idxs[:,1:].reshape(-1)\n",
    "selected_idxs = np.unique(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 297)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(selected_idxs, noise_idxs)), len(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = samples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(tr)\n",
    "_, idxs = nbrs.kneighbors(tr[selected])\n",
    "selected_idxs = idxs[:,1:].reshape(-1)\n",
    "selected_idxs = np.unique(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 297)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(selected_idxs, noise_idxs)), len(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.choice(50000,size=300,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 297)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.intersect1d(selected, noise_idxs)), len(selected_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/xianglin/projects/DVI_data/noisy/{}/cifar10/embedding.npy\".format(dataset)\n",
    "embeddings_2d = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/xianglin/projects/DVI_data/resnet18_cifar10/embedding.npy\"\n",
    "embeddings_2d = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import Birch, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_centroid(samples, n_select=3):\n",
    "    kmeans = KMeans(n_clusters=n_select).fit(samples)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(samples)\n",
    "    indices = nbrs.kneighbors(kmeans.cluster_centers_,return_distance=False)\n",
    "    return indices.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_abnormal(directions, repeat=2, show=True, dr=\"umap\"):\n",
    "    for _ in range(repeat):\n",
    "        if dr==\"umap\":\n",
    "            reducer = umap.UMAP(n_components=2)\n",
    "            embedding = reducer.fit_transform(directions)\n",
    "        elif dr==\"tsne\":\n",
    "            embedding = TSNE(n_components=2, learning_rate='auto', init='pca').fit_transform(high_data)\n",
    "        else:\n",
    "            print(\"illgal dr algorithm!\")\n",
    "            return False\n",
    "\n",
    "        brc = Birch(n_clusters=2)\n",
    "        brc.fit(embedding)\n",
    "\n",
    "        s = silhouette_score(embedding, brc.labels_, metric='euclidean')\n",
    "        c = calinski_harabasz_score(embedding, brc.labels_)\n",
    "        if s <= 0.1:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    if s <= 0.1:\n",
    "        print(\"No abnormal trajectories detected!\")\n",
    "        return False\n",
    "    print(\"silhouette_score\\t\", s)\n",
    "    print(\"calinski_harabasz_score\\t\", c)\n",
    "\n",
    "    labels = brc.labels_\n",
    "    centroid = brc.subcluster_centers_\n",
    "    centroid_labels = brc.subcluster_labels_\n",
    "    # clean 1, noise 0\n",
    "    bin = np.bincount(centroid_labels)\n",
    "    if bin[0] > bin[1]:\n",
    "        centroid_labels = np.abs(centroid_labels-1)\n",
    "        labels = np.abs(labels-1)\n",
    "    \n",
    "    # select 3 representative clean ones\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(embedding)\n",
    "    indices = nbrs.kneighbors(centroid, return_distance=False)\n",
    "    centroid = embedding[indices.squeeze()]\n",
    "\n",
    "    # calculate noise score\n",
    "    clean_centroids = centroid[centroid_labels==1]\n",
    "    clean_ones = embedding[labels==1]\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(clean_centroids)\n",
    "    dists, _ = nbrs.kneighbors(clean_ones)\n",
    "    norm_term = dists.max()\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(clean_centroids)\n",
    "    dists, _ = nbrs.kneighbors(embedding)\n",
    "    scores = dists/norm_term\n",
    "    \n",
    "    if show:\n",
    "        plt.scatter(\n",
    "            embedding[:, 0],\n",
    "            embedding[:, 1],\n",
    "            s=1,\n",
    "            c=labels,\n",
    "            cmap=\"Pastel2\")\n",
    "\n",
    "        c_idxs = select_centroid(clean_centroids)\n",
    "        plt.scatter(\n",
    "            clean_centroids[c_idxs][:, 0],\n",
    "            clean_centroids[c_idxs][:, 1],\n",
    "            s=4,\n",
    "            c=\"black\",\n",
    "            )\n",
    "        plt.scatter(\n",
    "            centroid[centroid_labels==0][:, 0],\n",
    "            centroid[centroid_labels==0][:, 1],\n",
    "            s=4,\n",
    "            c=\"red\",\n",
    "            )\n",
    "        plt.title('Trajectories Visualization', fontsize=24)\n",
    "        plt.show()\n",
    "\n",
    "    return labels, scores, centroid, centroid_labels, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataset\n",
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(train_labels == cls_num).squeeze()\n",
    "    # lbs = np.array(train_labels)[cls]\n",
    "    high_data = directions[cls].reshape(len(cls), -1)\n",
    "    test_abnormal(high_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataset\n",
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(train_labels == cls_num).squeeze()\n",
    "    # lbs = np.array(train_labels)[cls]\n",
    "    high_data = embeddings_2d[cls].reshape(len(cls), -1)\n",
    "    _,_,_,_,embedding = test_abnormal(high_data)\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=train_labels[cls],\n",
    "        cmap=\"tab10\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairflip\n",
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "    # lbs = np.array(clean_label)[cls]\n",
    "    high_data = embeddings_2d[cls].reshape(len(cls), -1)\n",
    "    _,_,_,_,embedding = test_abnormal(high_data)\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=clean_label[cls],\n",
    "        cmap=\"tab10\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetric\n",
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "    # lbs = np.array(clean_label)[cls]\n",
    "    high_data = embeddings_2d[cls].reshape(len(cls), -1)\n",
    "    _,_,_,_,embedding = test_abnormal(high_data, dr=\"umap\")\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=clean_label[cls],\n",
    "        cmap=\"tab10\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noise score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_num = 1\n",
    "cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "high_data = directions[cls].reshape(len(cls), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_num = 1\n",
    "cls = np.argwhere(train_labels == cls_num).squeeze()\n",
    "high_data = directions[cls].reshape(len(cls), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2)\n",
    "embedding = reducer.fit_transform(high_data)\n",
    "\n",
    "brc = Birch(n_clusters=2)\n",
    "brc.fit(embedding)\n",
    "\n",
    "s = silhouette_score(embedding, brc.labels_, metric='euclidean')\n",
    "c = calinski_harabasz_score(embedding, brc.labels_)\n",
    "print(\"silhouette_score\\t\", s)\n",
    "print(\"calinski_harabasz_score\\t\", c)\n",
    "\n",
    "labels = brc.labels_\n",
    "centroid = brc.subcluster_centers_\n",
    "centroid_labels = brc.subcluster_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin = np.bincount(centroid_labels)\n",
    "if bin[0]>bin[1]:\n",
    "    centroid_labels = np.abs(centroid_labels-1)\n",
    "# 0 is noise\n",
    "bin = np.bincount(labels)\n",
    "if bin[0]>bin[1]:\n",
    "    labels = np.abs(labels-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ones = embedding[labels==1]\n",
    "noise_ones = embedding[labels==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(centroid[centroid_labels==1])\n",
    "dists, indices = nbrs.kneighbors(clean_ones)\n",
    "norm = dists.max()\n",
    "norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, indices = nbrs.kneighbors(noise_ones)\n",
    "normed = dists/norm\n",
    "normed.max(), normed.min(), normed.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "    high_data = embeddings_2d[cls].reshape(len(cls), -1)\n",
    "\n",
    "\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    embedding = reducer.fit_transform(high_data)\n",
    "\n",
    "    brc = Birch(n_clusters=2)\n",
    "    brc.fit(embedding)\n",
    "\n",
    "    labels = brc.labels_\n",
    "    centroid = brc.subcluster_centers_\n",
    "    centroid_labels = brc.subcluster_labels_\n",
    "    # clean 1, noise 0\n",
    "    bin = np.bincount(labels)\n",
    "    if bin[0] > bin[1]:\n",
    "        centroid_labels = np.abs(centroid_labels-1)\n",
    "        labels = np.abs(labels-1)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=clean_label[cls],\n",
    "        cmap=\"tab10\")\n",
    "    plt.scatter(\n",
    "        brc.subcluster_centers_[:, 0],\n",
    "        brc.subcluster_centers_[:, 1],\n",
    "        s=5,\n",
    "        c='black')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=brc.labels_,\n",
    "        cmap=\"Pastel2\")\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(centroid[centroid_labels==1])\n",
    "    dists, indices = nbrs.kneighbors(centroid[centroid_labels==1])\n",
    "    suspicious = (dists[:, -1]/ dists[:, 1])>1.8\n",
    "\n",
    "    cleans = centroid[centroid_labels==1]\n",
    "    noises = centroid[centroid_labels==0]\n",
    "    plt.scatter(\n",
    "        cleans[:, 0],\n",
    "        cleans[:, 1],\n",
    "        s=5,\n",
    "        c='r')\n",
    "    plt.scatter(\n",
    "        noises[:, 0],\n",
    "        noises[:, 1],\n",
    "        s=5,\n",
    "        c='black')\n",
    "    plt.scatter(\n",
    "        cleans[suspicious][:, 0],\n",
    "        cleans[suspicious][:, 1],\n",
    "        s=5,\n",
    "        c='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 250\n",
    "embedding_2d = embeddings_2d[:,s:]\n",
    "l = (400-s)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "    high_data = embedding_2d[cls].reshape(len(cls), -1)\n",
    "\n",
    "    # umap reduction\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    embedding = reducer.fit_transform(high_data)\n",
    "\n",
    "    # PCA\n",
    "    _, _, v = np.linalg.svd(high_data)\n",
    "    scores = np.abs(np.inner(v[0], high_data))\n",
    "    m = embedding.mean()\n",
    "    scores = scores / scores.max()\n",
    "\n",
    "    brc = Birch(n_clusters=2)\n",
    "    brc.fit(embedding)\n",
    "    s = silhouette_score(embedding, brc.labels_, metric='euclidean')\n",
    "    c = calinski_harabasz_score(embedding, brc.labels_)\n",
    "    print(\"silhouette_score\\t\", s)\n",
    "    print(\"calinski_harabasz_score\\t\", c)\n",
    "    labels = brc.labels_\n",
    "    centroid = brc.subcluster_centers_\n",
    "    centroid_labels = brc.subcluster_labels_\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(embedding)\n",
    "    _, idxs = nbrs.kneighbors(centroid)\n",
    "    centroid = embedding[idxs].squeeze(axis=1)\n",
    "\n",
    "    # clean 0, noise 1\n",
    "    if np.sum(labels==0)<np.sum(labels==1):\n",
    "        labels = np.abs(1-labels)\n",
    "        centroid_labels = np.abs(1-centroid_labels)\n",
    "\n",
    "    clean_center = embedding[labels==0].mean(axis=0)\n",
    "\n",
    "    X_plot = np.linspace(0, 1, 1000)[:, np.newaxis]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(scores.reshape(len(scores), 1))\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    i = np.argmax(np.exp(log_dens))\n",
    "    dense = X_plot[i, 0]\n",
    "\n",
    "    # ground truth\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111,)\n",
    "    ax.scatter(\n",
    "        embedding[:,0],\n",
    "        embedding[:,1],\n",
    "        s=.1,\n",
    "        c=clean_label[cls],\n",
    "        cmap=\"tab10\")\n",
    "    ax.scatter(\n",
    "        centroid[:, 0],\n",
    "        centroid[:, 1],\n",
    "        s=5,\n",
    "        c=\"black\")\n",
    "    plt.show()\n",
    "\n",
    "    pca_scores = np.abs(scores[idxs]-dense).squeeze()\n",
    "\n",
    "    clean_centers = np.array([clean_center])\n",
    "    umap_scores = np.linalg.norm(centroid-clean_center, axis=1)\n",
    "    umap_scores = umap_scores/umap_scores.max()\n",
    "\n",
    "    s = umap_scores+pca_scores\n",
    "\n",
    "    # hybrid scoring\n",
    "    l=np.zeros(len(centroid))\n",
    "    l[s>.5] = 1\n",
    "    plt.scatter(\n",
    "        centroid[:, 0],\n",
    "        centroid[:, 1],\n",
    "        s=5,\n",
    "        c=s,\n",
    "        cmap=\"Reds\"\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.scatter(\n",
    "        centroid[:, 0],\n",
    "        centroid[:, 1],\n",
    "        s=5,\n",
    "        c=l,\n",
    "        cmap=\"tab10\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "embedding_2d = embeddings_2d[:,s:]\n",
    "embeddings_2d.shape,embedding_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "    high_data = embedding_2d[cls].reshape(len(cls), -1)\n",
    "\n",
    "    # umap reduction\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    embedding = reducer.fit_transform(high_data)\n",
    "    \n",
    "    # PCA\n",
    "    _, _, v = np.linalg.svd(high_data)\n",
    "    scores = np.abs(np.inner(v[0], high_data))\n",
    "    m = embedding.mean()\n",
    "    scores = scores / scores.max()\n",
    "\n",
    "    brc = Birch(n_clusters=2)\n",
    "    brc.fit(embedding)\n",
    "    s = silhouette_score(embedding, brc.labels_, metric='euclidean')\n",
    "    c = calinski_harabasz_score(embedding, brc.labels_)\n",
    "    print(\"silhouette_score\\t\", s)\n",
    "    print(\"calinski_harabasz_score\\t\", c)\n",
    "    labels = brc.labels_\n",
    "    centroid = brc.subcluster_centers_\n",
    "    centroid_labels = brc.subcluster_labels_\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(embedding)\n",
    "    _, idxs = nbrs.kneighbors(centroid)\n",
    "    centroid = embedding[idxs].squeeze(axis=1)\n",
    "\n",
    "    # clean 0, noise 1\n",
    "    if np.sum(labels==0)<np.sum(labels==1):\n",
    "        labels = np.abs(1-labels)\n",
    "        centroid_labels = np.abs(1-centroid_labels)\n",
    "    \n",
    "    clean_center = embedding[labels==0].mean(axis=0)\n",
    "\n",
    "    X_plot = np.linspace(0, 1, 1000)[:, np.newaxis]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(scores.reshape(len(scores), 1))\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    i = np.argmax(np.exp(log_dens))\n",
    "    dense = np.exp(log_dens)[i]\n",
    "\n",
    "    # ground truth\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111,)\n",
    "    ax.scatter(\n",
    "        embedding[:,0],\n",
    "        embedding[:,1],\n",
    "        s=.1,\n",
    "        c=clean_label[cls],\n",
    "        cmap=\"tab10\")\n",
    "    ax.scatter(\n",
    "        centroid[:, 0],\n",
    "        centroid[:, 1],\n",
    "        s=5,\n",
    "        c=\"black\")\n",
    "    plt.show()\n",
    "\n",
    "    umap_scores = np.linalg.norm(centroid-clean_center, axis=1)\n",
    "    umap_scores = umap_scores/umap_scores.max()\n",
    "\n",
    "    log_dens = kde.score_samples(scores.reshape(len(scores), 1))[idxs]\n",
    "    pca_scores = np.abs(np.exp(log_dens)-dense).squeeze(axis=1)\n",
    "\n",
    "    clean_scores = pca_scores[np.argwhere(centroid_labels==0)].squeeze(axis=1)\n",
    "    detect_n = (clean_scores-clean_scores.mean())/clean_scores.std()\n",
    "    susp_ones = np.argwhere(np.abs(detect_n)>2)\n",
    "    s = umap_scores+pca_scores\n",
    "    print(s)\n",
    "\n",
    "    # umap clustering\n",
    "    plt.scatter(\n",
    "        centroid[:, 0],\n",
    "        centroid[:, 1],\n",
    "        s=5,\n",
    "        c=centroid_labels,\n",
    "        cmap=\"tab10\"\n",
    "    )\n",
    "    plt.show()\n",
    "    # hybrid scoring\n",
    "    # l = np.copy(centroid_labels)\n",
    "    # l[np.argwhere(centroid_labels==0)[susp_ones]] = 2\n",
    "    l=np.zeros(len(centroid))\n",
    "    l[s>.5] = 1\n",
    "    plt.scatter(\n",
    "        centroid[:, 0],\n",
    "        centroid[:, 1],\n",
    "        s=5,\n",
    "        c=s,\n",
    "        cmap=\"Reds\"\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.scatter(\n",
    "        centroid[:, 0],\n",
    "        centroid[:, 1],\n",
    "        s=5,\n",
    "        c=l,\n",
    "        cmap=\"tab10\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 85\n",
    "embedding_2d = embeddings_2d[:,s:]\n",
    "embeddings_2d.shape,embedding_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls_num in range(10):\n",
    "\n",
    "    cls = np.argwhere(np.array(train_labels)==cls_num).squeeze()\n",
    "    high_data = embedding_2d[cls].reshape(len(cls), -1)\n",
    "\n",
    "    _, _, v = np.linalg.svd(high_data)\n",
    "    scores = np.abs(np.inner(v[0], high_data))\n",
    "    scores = scores / scores.max()\n",
    "\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    embedding = reducer.fit_transform(high_data)\n",
    "    brc = Birch(n_clusters=2)\n",
    "    brc.fit(embedding)\n",
    "    s = silhouette_score(embedding, brc.labels_, metric='euclidean')\n",
    "    c = calinski_harabasz_score(embedding, brc.labels_)\n",
    "    print(\"silhouette_score\\t\", s)\n",
    "    print(\"calinski_harabasz_score\\t\", c)\n",
    "    # labels = brc.labels_\n",
    "    # centroid = brc.subcluster_centers_\n",
    "    # centroid_labels = brc.subcluster_labels_\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111,)\n",
    "\n",
    "    ax.scatter(\n",
    "        embedding[:,0],\n",
    "        embedding[:,1],\n",
    "        # scores,\n",
    "        s=.1,\n",
    "        c=train_labels[cls],\n",
    "        cmap=\"tab10\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.scatter(\n",
    "        embedding[:,0],\n",
    "        scores,\n",
    "        s=.1,\n",
    "        c=train_labels[cls],\n",
    "        cmap=\"tab10\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, v = np.linalg.svd(high_data)\n",
    "scores2 = np.abs(np.inner(v[1], high_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "reducer = PCA(n_components=1)\n",
    "s1 = -reducer.fit_transform(high_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=1)\n",
    "s2 = -reducer.fit_transform(high_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(scores.squeeze(), s1.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "feats_ = np.ravel(scores1).astype(float).reshape(-1, 1)\n",
    "gmm = GMM(n_components=2, covariance_type='full', tol=1e-6, max_iter=100)\n",
    "gmm.fit(feats_)\n",
    "prob = gmm.predict_proba(feats_)\n",
    "cl = gmm.predict(feats_)\n",
    "# prob = prob[:,gmm.means_.argmax()]\n",
    "means = gmm.means_\n",
    "import scipy\n",
    "centers = np.zeros(2, dtype=np.int32)\n",
    "for i in range(gmm.n_components):\n",
    "    density = scipy.stats.multivariate_normal(cov=gmm.covariances_[i], mean=gmm.means_[i]).logpdf(feats_)\n",
    "    centers[i] = int(np.argmax(density))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2)\n",
    "embedding = reducer.fit_transform(high_data)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=clean_label[cls],\n",
    "    cmap=\"tab10\")\n",
    "plt.show()\n",
    "plt.title(\"svd\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=cl,\n",
    "    cmap=\"Greens\")\n",
    "plt.scatter(\n",
    "    embedding[centers][:, 0],\n",
    "    embedding[centers][:, 1],\n",
    "    s=5,\n",
    "    c=\"black\",\n",
    "    cmap=\"Greens\")\n",
    "plt.show()\n",
    "plt.title(\"svd\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=scores1,\n",
    "    cmap=\"Greens\")\n",
    "plt.scatter(\n",
    "    embedding[centers][:, 0],\n",
    "    embedding[centers][:, 1],\n",
    "    s=5,\n",
    "    c=\"black\",\n",
    "    cmap=\"Greens\")\n",
    "plt.show()\n",
    "plt.title(\"pca\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=s1,\n",
    "    cmap=\"Greens\")\n",
    "plt.scatter(\n",
    "    embedding[centers][:, 0],\n",
    "    embedding[centers][:, 1],\n",
    "    s=5,\n",
    "    c=\"black\",\n",
    "    cmap=\"Greens\")\n",
    "plt.show()\n",
    "plt.title(\"umap\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=s2,\n",
    "    cmap=\"Greens\")\n",
    "plt.scatter(\n",
    "    embedding[centers][:, 0],\n",
    "    embedding[centers][:, 1],\n",
    "    s=5,\n",
    "    c=\"black\",\n",
    "    cmap=\"Greens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2)\n",
    "embedding = reducer.fit_transform(high_data)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=clean_label[cls],\n",
    "    cmap=\"tab10\")\n",
    "plt.show()\n",
    "plt.title(\"svd\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=scores,\n",
    "    cmap=\"Greens\")\n",
    "plt.show()\n",
    "plt.title(\"pca\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=s1,\n",
    "    cmap=\"Greens\")\n",
    "plt.show()\n",
    "plt.title(\"umap\")\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=s2,\n",
    "    cmap=\"Greens\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=10)\n",
    "embedding = reducer.fit_transform(high_data)\n",
    "\n",
    "feats_ = np.concatenate((scores.reshape(len(scores), 1), embedding), axis=1)\n",
    "from sklearn import cluster\n",
    "kmeans = cluster.KMeans(n_clusters=2, random_state=0).fit(feats_)\n",
    "if np.mean(scores[kmeans.labels_==0]) < np.mean(scores[kmeans.labels_==1]): kmeans.labels_ = 1 - kmeans.labels_  \n",
    "\n",
    "reducer = PCA(n_components=2)\n",
    "embedding = reducer.fit_transform(high_data)    \n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=clean_label[cls],\n",
    "    cmap=\"tab10\")\n",
    "plt.show()\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=kmeans.labels_,\n",
    "    cmap=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls_num in range(10):\n",
    "\n",
    "    cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "    high_data = embeddings_2d[cls].reshape(len(cls), -1)\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    # G = high_data.T.dot(high_data)\n",
    "\n",
    "    reducer = PCA(n_components=2)\n",
    "    embedding = reducer.fit_transform(high_data)\n",
    "    eigen_vec = reducer.components_\n",
    "    scores = cosine_similarity(eigen_vec,high_data)\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gm = GaussianMixture(n_components=2, random_state=0).fit(embedding)\n",
    "    labels = gm.predict(embedding)\n",
    "    prob = gm.predict_proba(embedding)\n",
    "\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=clean_label[cls],\n",
    "        cmap=\"tab10\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=prob[:,0],\n",
    "        cmap=\"Greens\")\n",
    "    plt.show()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=prob[:,1],\n",
    "        cmap=\"Reds\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #     # reducer = umap.UMAP(n_components=2)\n",
    "    #     # embedding = reducer.fit_transform(high_data)\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     embedding[:, 0],\n",
    "    #     embedding[:, 1],\n",
    "    #     s=1,\n",
    "    #     c=similarity[0],\n",
    "    #     cmap=\"Greens\")\n",
    "\n",
    "    # plt.show()\n",
    "    #     # plt.scatter(\n",
    "    #     #     embedding[:, 0],\n",
    "    #     #     embedding[:, 1],\n",
    "    #     #     s=1,\n",
    "    #     #     c=clean_label[cls],\n",
    "    #     #     cmap=\"tab10\")\n",
    "    #     # plt.show()\n",
    "\n",
    "    \n",
    "    # plt.scatter(\n",
    "    #     embedding[:, 0],\n",
    "    #     embedding[:, 1],\n",
    "    #     s=1,\n",
    "    #     c=labels,\n",
    "    #     cmap=\"tab10\")\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(np.array(noisy_label)==cls_num).squeeze()\n",
    "    high_data = embeddings_2d[cls].reshape(len(cls), -1)\n",
    "\n",
    "\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    embedding = reducer.fit_transform(high_data)\n",
    "\n",
    "    # from sklearn.manifold import TSNE\n",
    "    # embedding = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(high_data)\n",
    "\n",
    "    brc = Birch(n_clusters=2)\n",
    "    brc.fit(embedding)\n",
    "\n",
    "    labels = brc.labels_\n",
    "    centroid = brc.subcluster_centers_\n",
    "    centroid_labels = brc.subcluster_labels_\n",
    "    # clean 1, noise 0\n",
    "    bin = np.bincount(labels)\n",
    "    if bin[0] > bin[1]:\n",
    "        centroid_labels = np.abs(centroid_labels-1)\n",
    "        labels = np.abs(labels-1)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=clean_label[cls],\n",
    "        cmap=\"tab10\")\n",
    "    plt.scatter(\n",
    "        brc.subcluster_centers_[:, 0],\n",
    "        brc.subcluster_centers_[:, 1],\n",
    "        s=5,\n",
    "        c='black')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=brc.labels_,\n",
    "        cmap=\"Pastel2\")\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(centroid[centroid_labels==1])\n",
    "    dists, indices = nbrs.kneighbors(centroid[centroid_labels==1])\n",
    "    suspicious = (dists[:, -1]/ dists[:, 1])>1.8\n",
    "\n",
    "    cleans = centroid[centroid_labels==1]\n",
    "    noises = centroid[centroid_labels==0]\n",
    "    plt.scatter(\n",
    "        cleans[:, 0],\n",
    "        cleans[:, 1],\n",
    "        s=5,\n",
    "        c='r')\n",
    "    plt.scatter(\n",
    "        noises[:, 0],\n",
    "        noises[:, 1],\n",
    "        s=5,\n",
    "        c='black')\n",
    "    plt.scatter(\n",
    "        cleans[suspicious][:, 0],\n",
    "        cleans[suspicious][:, 1],\n",
    "        s=5,\n",
    "        c='b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls_num in range(10):\n",
    "    cls = np.argwhere(np.array(train_labels)==cls_num).squeeze()\n",
    "    high_data = embeddings_2d[cls].reshape(len(cls), -1)\n",
    "\n",
    "\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    embedding = reducer.fit_transform(high_data)\n",
    "\n",
    "    # from sklearn.manifold import TSNE\n",
    "    # embedding = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(high_data)\n",
    "\n",
    "    brc = Birch(n_clusters=2)\n",
    "    brc.fit(embedding)\n",
    "\n",
    "    labels = brc.labels_\n",
    "    centroid = brc.subcluster_centers_\n",
    "    centroid_labels = brc.subcluster_labels_\n",
    "    # clean 1, noise 0\n",
    "    bin = np.bincount(labels)\n",
    "    if bin[0] > bin[1]:\n",
    "        centroid_labels = np.abs(centroid_labels-1)\n",
    "        labels = np.abs(labels-1)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=train_labels[cls],\n",
    "        cmap=\"tab10\")\n",
    "    plt.scatter(\n",
    "        brc.subcluster_centers_[:, 0],\n",
    "        brc.subcluster_centers_[:, 1],\n",
    "        s=5,\n",
    "        c='black')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        s=1,\n",
    "        c=brc.labels_,\n",
    "        cmap=\"Pastel2\")\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(centroid[centroid_labels==1])\n",
    "    dists, indices = nbrs.kneighbors(centroid[centroid_labels==1])\n",
    "    suspicious = (dists[:, -1]/ dists[:, 1])>1.8\n",
    "\n",
    "    cleans = centroid[centroid_labels==1]\n",
    "    noises = centroid[centroid_labels==0]\n",
    "    plt.scatter(\n",
    "        cleans[:, 0],\n",
    "        cleans[:, 1],\n",
    "        s=5,\n",
    "        c='r')\n",
    "    plt.scatter(\n",
    "        noises[:, 0],\n",
    "        noises[:, 1],\n",
    "        s=5,\n",
    "        c='black')\n",
    "    plt.scatter(\n",
    "        cleans[suspicious][:, 0],\n",
    "        cleans[suspicious][:, 1],\n",
    "        s=5,\n",
    "        c='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1,\n",
    "    c=brc.labels_,\n",
    "    cmap=\"Pastel2\")\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(centroid[centroid_labels==1])\n",
    "dists, indices = nbrs.kneighbors(centroid[centroid_labels==1])\n",
    "suspicious = (dists[:, -1]/ dists[:, 1])>1.8\n",
    "\n",
    "cleans = centroid[centroid_labels==1]\n",
    "noises = centroid[centroid_labels==0]\n",
    "plt.scatter(\n",
    "    cleans[:, 0],\n",
    "    cleans[:, 1],\n",
    "    s=5,\n",
    "    c='r')\n",
    "plt.scatter(\n",
    "    noises[:, 0],\n",
    "    noises[:, 1],\n",
    "    s=5,\n",
    "    c='black')\n",
    "plt.scatter(\n",
    "    cleans[suspicious][:, 0],\n",
    "    cleans[suspicious][:, 1],\n",
    "    s=5,\n",
    "    c='b')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa7a9f36e1a1e240450dbe9cc8f6d8df1d5301f36681fb271c44fdd883236b60"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('SV': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
