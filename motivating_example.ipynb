{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding motivating examples for SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from umap.umap_ import find_ab_params\n",
    "\n",
    "from singleVis.SingleVisualizationModel import SingleVisualizationModel\n",
    "from singleVis.losses import SingleVisLoss, UmapLoss, ReconstructionLoss\n",
    "from singleVis.trainer import SingleVisTrainer\n",
    "from singleVis.data import DataProvider\n",
    "from singleVis.visualizer import visualizer\n",
    "import singleVis.config as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"cifar10\"\n",
    "CONTENT_PATH = \"/home/xianglin/projects/DVI_data/TemporalExp/resnet18_{}\".format(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LEN = config.dataset_config[DATASET][\"TRAINING_LEN\"]\n",
    "LAMBDA = config.dataset_config[DATASET][\"LAMBDA\"]\n",
    "\n",
    "# define hyperparameters\n",
    "\n",
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATIENT = config.dataset_config[DATASET][\"training_config\"][\"PATIENT\"]\n",
    "EPOCH_START = config.dataset_config[DATASET][\"EPOCH_START\"]\n",
    "EPOCH_END = config.dataset_config[DATASET][\"EPOCH_END\"]\n",
    "EPOCH_PERIOD = config.dataset_config[DATASET][\"EPOCH_PERIOD\"]\n",
    "\n",
    "content_path = CONTENT_PATH\n",
    "sys.path.append(content_path)\n",
    "\n",
    "from Model.model import *\n",
    "net = resnet18()\n",
    "classes = (\"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "\n",
    "data_provider = DataProvider(content_path, net, EPOCH_START, EPOCH_END, EPOCH_PERIOD, split=-1, device=DEVICE, verbose=1)\n",
    "model = SingleVisualizationModel(input_dims=512, output_dims=2, units=256)\n",
    "negative_sample_rate = 5\n",
    "min_dist = .1\n",
    "_a, _b = find_ab_params(1.0, min_dist)\n",
    "umap_loss_fn = UmapLoss(negative_sample_rate, DEVICE, _a, _b, repulsion_strength=1.0)\n",
    "recon_loss_fn = ReconstructionLoss(beta=1.0)\n",
    "criterion = SingleVisLoss(umap_loss_fn, recon_loss_fn, lambd=LAMBDA)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.01, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=.1)\n",
    "\n",
    "trainer = SingleVisTrainer(model, criterion, optimizer, lr_scheduler, edge_loader=None, DEVICE=DEVICE)\n",
    "trainer.load(file_path=os.path.join(data_provider.model_path,\"tnn.pth\"))\n",
    "# trainer.load(file_path=os.path.join(data_provider.model_path,\"motivated_SV.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a stable sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_data = data_provider.train_representation(8)\n",
    "curr_data = data_provider.train_representation(9)\n",
    "\n",
    "dists = np.linalg.norm(prev_data - curr_data, axis=1)\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.min(), dists.mean(), dists.max(), np.argmin(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from singleVis.backend import find_neighbor_preserving_rate\n",
    "\n",
    "npr = find_neighbor_preserving_rate(prev_data, curr_data, n_neighbors=15)\n",
    "npr[7450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(npr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_list = [7450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = visualizer(data_provider, trainer.model, 200, 10, classes)\n",
    "save_dir = os.path.join(data_provider.content_path, \"img\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "for i in [1,4,10]:\n",
    "    curr_data = data_provider.train_representation(i)\n",
    "    pred = data_provider.get_pred(i, curr_data)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    labels = data_provider.train_labels(i)\n",
    "    # vis.savefig_cus(i, curr_data[vis_list], pred[vis_list], labels[vis_list], path=os.path.join(save_dir,\"motivated_{}_{}.png\".format(DATASET, i)))\n",
    "    vis.savefig_cus(i, curr_data, pred, labels, path=os.path.join(save_dir,\"motivated_{}_{}.png\".format(DATASET, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = visualizer(data_provider, trainer.model, 200, 10, classes)\n",
    "save_dir = os.path.join(data_provider.content_path, \"img\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "for i in range(5, 12, 1):\n",
    "    curr_data = data_provider.train_representation(i)\n",
    "    pred = data_provider.get_pred(i, curr_data)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    labels = data_provider.train_labels(i)\n",
    "\n",
    "    prev_data = data_provider.train_representation(i-1)\n",
    "    prev_pred = data_provider.get_pred(i-1, prev_data)\n",
    "    prev_pred = np.argmax(prev_pred, axis=1)\n",
    "    prev_labels = data_provider.train_labels(i-1)\n",
    "\n",
    "    vis.savefig_trajectory(i, prev_data[vis_list], prev_pred[vis_list], prev_labels[vis_list], curr_data[vis_list], pred[vis_list], labels[vis_list], path=os.path.join(save_dir,\"motivated_{}_{}.png\".format(DATASET, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = None\n",
    "embeddings = torch.Tensor([]).to(\"cpu\")\n",
    "for i in range(1, TIME_STEPS+1, 1):\n",
    "    data = data_provider.train_representation(i)\n",
    "    if all_data is None:\n",
    "        all_data = data[:, None,:]\n",
    "    else:\n",
    "        all_data = np.concatenate((all_data, data[:, None, :]), axis=1)\n",
    "    data = torch.from_numpy(data).to(device=data_provider.DEVICE, dtype=torch.float)\n",
    "    embedding = trainer.model.encoder(data).detach().cpu()\n",
    "    embeddings = torch.concat((embeddings, embedding[:, None, :]), dim=1)\n",
    "embeddings = embeddings.numpy()\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import singleVis.trajectory as trajectory\n",
    "# trajectory.draw_trajectory(embeddings[7450][4:],x_min=-6.0, x_max=-2.5, y_min=5.5, y_max=7.1)\n",
    "trajectory.draw_trajectory(embeddings[7450][4:], x_min=-4, x_max=-1, y_min=-4.1, y_max=-2.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth of dists change and direction change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape\n",
    "high_dists = np.zeros((50000, 10))\n",
    "anchor = 8\n",
    "for i in range(50000):\n",
    "    for j in range(10):\n",
    "        high_dists[i][j] = np.linalg.norm(all_data[i][anchor]-all_data[i][j])\n",
    "low_dists = np.zeros((50000, 10))\n",
    "for i in range(50000):\n",
    "    for j in range(10):\n",
    "        low_dists[i][j] = np.linalg.norm(embeddings[i][anchor]-embeddings[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dists[101],low_dists[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "high_directions = np.zeros((50000, 9))\n",
    "for i in range(50000):\n",
    "    for j in range(9):\n",
    "        high_directions[i][j] = 1 - distance.cosine(all_data[i][j]-all_data[i][j+1], all_data[i][j+1]-all_data[i][j+2])\n",
    "low_directions = np.zeros((50000, 9))\n",
    "for i in range(50000):\n",
    "    for j in range(9):\n",
    "        low_directions[i][j] = 1 - distance.cosine(embeddings[i][j]-embeddings[i][j+1],embeddings[i][j+1]-embeddings[i][j+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_directions[7450],low_directions[7450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation betweem high dim dists and low dim dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_data = data_provider.train_representation(9)\n",
    "curr_data = data_provider.train_representation(11)\n",
    "\n",
    "dists = np.linalg.norm(prev_data - curr_data, axis=1)\n",
    "\n",
    "prev_data = torch.from_numpy(prev_data).to(device=data_provider.DEVICE, dtype=torch.float)\n",
    "prev_embedding = trainer.model.encoder(prev_data).detach().cpu().numpy()\n",
    "\n",
    "curr_data = torch.from_numpy(curr_data).to(device=data_provider.DEVICE, dtype=torch.float)\n",
    "curr_embedding = trainer.model.encoder(curr_data).detach().cpu().numpy()\n",
    "\n",
    "embedding_dists = np.linalg.norm(prev_embedding-curr_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "corr = pearsonr(dists, embedding_dists)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa7a9f36e1a1e240450dbe9cc8f6d8df1d5301f36681fb271c44fdd883236b60"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('SV': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
