{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding motivating examples for SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from umap.umap_ import find_ab_params\n",
    "\n",
    "from singleVis.SingleVisualizationModel import SingleVisualizationModel\n",
    "from singleVis.losses import SingleVisLoss, UmapLoss, ReconstructionLoss\n",
    "from singleVis.trainer import SingleVisTrainer\n",
    "from singleVis.data import DataProvider\n",
    "from singleVis.visualizer import visualizer\n",
    "import singleVis.config as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"cifar10\"\n",
    "CONTENT_PATH = \"/home/xianglin/projects/DVI_data/TemporalExp/resnet18_{}\".format(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish initialization...\n",
      "Successfully load visualization model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LEN = config.dataset_config[DATASET][\"TRAINING_LEN\"]\n",
    "LAMBDA = config.dataset_config[DATASET][\"LAMBDA\"]\n",
    "\n",
    "# define hyperparameters\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCH_NUMS = config.dataset_config[DATASET][\"training_config\"][\"EPOCH_NUM\"]\n",
    "TIME_STEPS = config.dataset_config[DATASET][\"training_config\"][\"TIME_STEPS\"]\n",
    "TEMPORAL_PERSISTENT = config.dataset_config[DATASET][\"training_config\"][\"TEMPORAL_PERSISTENT\"]\n",
    "NUMS = config.dataset_config[DATASET][\"training_config\"][\"NUMS\"]    # how many epoch should we go through for one pass\n",
    "PATIENT = config.dataset_config[DATASET][\"training_config\"][\"PATIENT\"]\n",
    "\n",
    "content_path = CONTENT_PATH\n",
    "sys.path.append(content_path)\n",
    "\n",
    "from Model.model import *\n",
    "net = resnet18()\n",
    "classes = (\"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "\n",
    "data_provider = DataProvider(content_path, net, 1, TIME_STEPS, 1, split=-1, device=DEVICE, verbose=1)\n",
    "model = SingleVisualizationModel(input_dims=512, output_dims=2, units=256)\n",
    "negative_sample_rate = 5\n",
    "min_dist = .1\n",
    "_a, _b = find_ab_params(1.0, min_dist)\n",
    "umap_loss_fn = UmapLoss(negative_sample_rate, DEVICE, _a, _b, repulsion_strength=1.0)\n",
    "recon_loss_fn = ReconstructionLoss(beta=1.0)\n",
    "criterion = SingleVisLoss(umap_loss_fn, recon_loss_fn, lambd=LAMBDA)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.01, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=.1)\n",
    "\n",
    "trainer = SingleVisTrainer(model, criterion, optimizer, lr_scheduler, edge_loader=None, DEVICE=DEVICE)\n",
    "trainer.load(file_path=os.path.join(data_provider.model_path,\"motivated_SV.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a stable sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_data = data_provider.train_representation(4)\n",
    "curr_data = data_provider.train_representation(11)\n",
    "\n",
    "dists = np.linalg.norm(prev_data - curr_data, axis=1)\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.min(), dists.mean(), dists.max(), np.argmin(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_list = [7450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = visualizer(data_provider, trainer.model, 200, 10, classes)\n",
    "save_dir = os.path.join(data_provider.content_path, \"img\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "for i in range(1, 12, 1):\n",
    "    curr_data = data_provider.train_representation(i)\n",
    "    pred = data_provider.get_pred(i, curr_data)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    labels = data_provider.train_labels(i)\n",
    "    vis.savefig_cus(i, curr_data[vis_list], pred[vis_list], labels[vis_list], path=os.path.join(save_dir,\"motivated_{}_{}.png\".format(DATASET, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5, 12, 1):\n",
    "    curr_data = data_provider.train_representation(i)\n",
    "    pred = data_provider.get_pred(i, curr_data)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    labels = data_provider.train_labels(i)\n",
    "\n",
    "    prev_data = data_provider.train_representation(i-1)\n",
    "    prev_pred = data_provider.get_pred(i-1, prev_data)\n",
    "    prev_pred = np.argmax(prev_pred, axis=1)\n",
    "    prev_labels = data_provider.train_labels(i-1)\n",
    "\n",
    "    vis.savefig_trajectory(i, prev_data[vis_list], prev_pred[vis_list], prev_labels[vis_list], curr_data[vis_list], pred[vis_list], labels[vis_list], path=os.path.join(save_dir,\"motivated_{}_{}.png\".format(DATASET, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 11, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = None\n",
    "embeddings = torch.Tensor([]).to(\"cpu\")\n",
    "for i in range(1, TIME_STEPS+1, 1):\n",
    "    data = data_provider.train_representation(i)\n",
    "    if all_data is None:\n",
    "        all_data = data[:, None,:]\n",
    "    else:\n",
    "        all_data = np.concatenate((all_data, data[:, None, :]), axis=1)\n",
    "    data = torch.from_numpy(data).to(device=data_provider.DEVICE, dtype=torch.float)\n",
    "    embedding = trainer.model.encoder(data).detach().cpu()\n",
    "    embeddings = torch.concat((embeddings, embedding[:, None, :]), dim=1)\n",
    "embeddings = embeddings.numpy()\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVmklEQVR4nO3df7BcZZ3n8fdnExk1sPIrBAaIoDKsO4uQVAM6y4Js1gD+AKkKiGXNMGhVjKXuDrWLolSJDrUWg1pbujqwWdQqXNBagQi7CyQZ1x+zKMoNQkACGBFMbulwFZUfWlDB7/5xT1LNze3cvsm9t2/Ofb+quvqc53lO9/cc0h/Offp0d6oKSVJ7/bNBFyBJml4GvSS1nEEvSS1n0EtSyxn0ktRyBr0ktdyEQZ/k2CT3dt2eSvI3Y8a8K8nGJPcn+V6S47v6Hmva700yNA37IEnahUzmOvok84Bh4OSqeryr/S+ATVX1myRnAR+vqpObvseATlX9akorlyT1Zf4kxy8Dftod8gBV9b2u1buAI/akqIMPPriOOuqoPXkISZpTNmzY8KuqWjhe32SD/gLgqxOMeQ9we9d6AeuSFPDfqmr1eBslWQmsBFi8eDFDQ87ySFK/kjzeq6/voE+yD3A28JFdjDmd0aA/pav5lKoaTnIIsD7JQ1X13bHbNv8DWA3Q6XT8XgZJmiKTuermLOCeqvqn8TqTvA64Fjinqn69vb2qhpv7J4A1wEm7X64kabImE/TvpMe0TZLFwM3AX1bVI13tC5Lst30ZWA48sPvlSpImq6+pmyak3wS8t6ttFUBVXQN8DDgI+PskANuqqgMsAtY0bfOBG6rqjqncAUnSrk3q8sqZ0ul0yjdjJal/STY0J9g78ZOxktRyBr0ktZxBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLWfQS1LLGfSS1HIGvSS1nEEvSS1n0EtSyxn0ktRyBr0ktZxBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLWfQS1LLTRj0SY5Ncm/X7akkfzNmTJJ8LsnmJBuTLO3quzDJT5rbhdOwD5KkXZg/0YCqehg4ASDJPGAYWDNm2FnAMc3tZOBq4OQkBwKXAx2ggA1Jbq2q30zVDkiSdm2yUzfLgJ9W1eNj2s8BrqtRdwH7JzkMOANYX1VPNuG+Hjhzj6uWJPVtskF/AfDVcdoPB7Z0rW9t2nq17yTJyiRDSYZGRkYmWZYkqZe+gz7JPsDZwNeno5CqWl1VnarqLFy4cDqeQpLmpMmc0Z8F3FNV/zRO3zBwZNf6EU1br3ZJ0gyZTNC/k/GnbQBuBf6qufrm9cDvquoXwFpgeZIDkhwALG/aJEkzZMKrbgCSLADeBLy3q20VQFVdA9wGvBnYDPweuKjpezLJFcDdzWZ/W1VPTln1kqQJpaoGXcNOOp1ODQ0NDboMSdprJNlQVZ3x+vxkrCS1nEEvSS1n0EtSyxn0ktRyBr0ktZxBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLWfQS1LLGfSS1HIGvSS1nEEvSS1n0EtSyxn0ktRyBr0ktZxBL0ktZ9BLUsv1++Pg+wPXAv8KKODdVfX9rv5LgHd1PeZrgYXNj4M/BjwNvABs6/WbhpKk6dFX0AOfBe6oqhVJ9gFe3t1ZVZ8CPgWQ5G3AxVX1ZNeQ06vqV1NRsCRpciYM+iSvAE4F/hqgqp4Hnt/FJu8EvjoVxUmS9lw/c/RHAyPAl5P8KMm1SRaMNzDJy4EzgZu6mgtYl2RDkpW9niTJyiRDSYZGRkYmsQuSpF3pJ+jnA0uBq6tqCfAscGmPsW8D7hwzbXNKVS0FzgLen+TU8TasqtVV1amqzsKFC/vfA0nSLvUT9FuBrVX1g2b9RkaDfzwXMGbapqqGm/sngDXASbtXqiRpd0wY9FX1S2BLkmObpmXAg2PHNXP5pwG3dLUtSLLf9mVgOfDAFNQtSepTv1fdfBC4vrni5lHgoiSrAKrqmmbMucC6qnq2a7tFwJok25/rhqq6Y0oqlyT1JVU16Bp20ul0amhoaNBlSNJeI8mGXp9T8pOxktRyBr0ktZxBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLWfQS1LLGfSS1HIGvSS1nEEvSS1n0EtSyxn0ktRyBr0ktZxBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLddX0CfZP8mNSR5KsinJG8b0vzHJ75Lc29w+1tV3ZpKHk2xOculU74Akadfm9znus8AdVbUiyT7Ay8cZ849V9dbuhiTzgC8AbwK2AncnubWqHtyToiVJ/ZvwjD7JK4BTgS8CVNXzVfXbPh//JGBzVT1aVc8DXwPO2c1aJUm7oZ+pm6OBEeDLSX6U5NokC8YZ94Yk9yW5PcmfN22HA1u6xmxt2naSZGWSoSRDIyMjk9kHSdIu9BP084GlwNVVtQR4Fhg7134P8MqqOh74r8A3JltIVa2uqk5VdRYuXDjZzSVJPfQT9FuBrVX1g2b9RkaDf4eqeqqqnmmWbwNekuRgYBg4smvoEU2bJGmGTBj0VfVLYEuSY5umZcCL3kxNcmiSNMsnNY/7a+Bu4JgkRzdv4l4A3DqF9UuSJtDvVTcfBK5vwvpR4KIkqwCq6hpgBfC+JNuAPwAXVFUB25J8AFgLzAO+VFU/nuqdkCT1ltE8nl06nU4NDQ0NugxJ2msk2VBVnfH6/GSsJLWcQS9JLWfQS1LLGfSS1HIGvSS1nEEvSS1n0EtSyxn0ktRyBr0ktZxBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLWfQS1LLGfSS1HIGvSS1nEGvOaWqWL9+Pe9973t57rnnBl2ONCP6/c1YqRVuuukmzjvvPACWLVvG+eefP+CKpOnX1xl9kv2T3JjkoSSbkrxhTP+7kmxMcn+S7yU5vqvvsab93iT+EKwG6i1veQv77rsvAJ/73OcGXI00M/qduvkscEdV/QvgeGDTmP6fAadV1XHAFcDqMf2nV9UJvX64VpopL3vZy1ixYgUAd955Jw888MCAK5Km34RBn+QVwKnAFwGq6vmq+m33mKr6XlX9plm9CzhiiuuUpsz73//+Hcvnn38+VTXAaqTp188Z/dHACPDlJD9Kcm2SBbsY/x7g9q71AtYl2ZBk5R7UKk2JTqfDvHnzANi0aROf/vSnB1yRNL36Cfr5wFLg6qpaAjwLXDrewCSnMxr0H+5qPqWqlgJnAe9PcmqPbVcmGUoyNDIyMpl9kCbtoIMO2rF86aWX8u1vf5tNmzZ5dq9W6ifotwJbq+oHzfqNjAb/iyR5HXAtcE5V/Xp7e1UNN/dPAGuAk8Z7kqpaXVWdquosXLhwcnshTdJRRx21Y/mPf/wjK1asYNmyZTz11FODK0qaJhMGfVX9EtiS5NimaRnwYPeYJIuBm4G/rKpHutoXJNlv+zKwHPDdLw3cYYcdRpId6wsWLOAXv/gF/jWpNur3qpsPAtcn2QicAHwyyaokq5r+jwEHAX8/5jLKRcD/S3If8EPg/1TVHVNXvjQ5VcUll1zCxo0bed3rXrej/ec//zmAQa9Wymyck+x0OjU05CX3mh7Dw8MsWbJk3FC/5ZZbOPvsswdQlbRnkmzodQm7X4GgOefwww/n61//+o4rb7p5Rq82Mug1J5122mlcddVVO7U/8cQTA6hGml4Gveasiy++mHe84x0varvzzjsHVI00fQx6zVlJuPbaa+m+nHft2rVs2jT2Gz6kvZtBrzlt3333ZdWqVTvWt23bxrnnnuv19GoVg15z3nHHHfei9YcffpiLLrrIT8mqNQx6zXnjfRL75ptvHvfNWmlvZNBrzuv1lRsf/ehH+eY3vznD1UhTz6DXnHfIIYfs1Hbcccfx4Q9/mHXr1vHMM88MoCpp6hj0mvMOPPBAkvDa1752x/ffPPLII3zyk59kwYIF/jiJ9noGvea8efPmsWTJEtauXcuRRx4JwHPPPcdHPvIRLr/8cj9Epb2eQS8BH/rQh/j+97/PJZdcsqPtyiuvBPy0rPZ+8wddgDQbnHDCCZx44ok8/fTTO/UZ9NrbeUYvAcceeyzXXXfduH0GvfZ2Br3UePvb385ll122U/vPfvazAVQjTR2DXuryiU98gjPOOONFbd/61rf4wx/+MKCKpD1n0Etd5s2bxw033MBLXvKSHW1PP/00q1at8isRtNcy6KUxDjzwQJYuXfqituuuu47Pf/7zA6pI2jMGvTSO17zmNTu1XXzxxXznO98ZQDXSnjHopXEsWrRop7YXXniB8847jy1btgygImn39RX0SfZPcmOSh5JsSvKGMf1J8rkkm5NsTLK0q+/CJD9pbhdO9Q5I02G8oP/KV77CF77wBR5//PEBVCTtvn4/MPVZ4I6qWpFkH+DlY/rPAo5pbicDVwMnJzkQuBzoAAVsSHJrVf1mSqqXpsmhhx4KjH5i9qqrruLEE0/k+OOP3+m766W9wYRn9EleAZwKfBGgqp6vqt+OGXYOcF2NugvYP8lhwBnA+qp6sgn39cCZU7kD0nRYtGgRn/nMZ7jyyivZvHkzP/zhDw157bX6OaM/GhgBvpzkeGAD8B+q6tmuMYcD3ROXW5u2Xu07SbISWAmwePHifuuXpsVpp53G8uXLScKrX/3qQZcj7ZF+5ujnA0uBq6tqCfAscOlUF1JVq6uqU1WdXj8EIc2Ul770pTu+slja2/UT9FuBrVX1g2b9RkaDv9swcGTX+hFNW692SdIMmTDoq+qXwJYkxzZNy4AHxwy7Ffir5uqb1wO/q6pfAGuB5UkOSHIAsLxpkyTNkH6vuvkgcH1zxc2jwEVJVgFU1TXAbcCbgc3A74GLmr4nk1wB3N08zt9W1ZNTWL8kaQKZjd/f0el0amhoaNBlSNJeI8mGquqM1+cnYyWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklqOYNeklrOoJekljPoJanlDHpJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklqOYNeklqurx8HT/IY8DTwArBt7O8SJrkEeFfXY74WWNj8OPgut5UkTa++gr5xelX9aryOqvoU8CmAJG8DLq6qJ/vZVpI0vaZj6uadwFen4XElSbuh36AvYF2SDUlW9hqU5OXAmcBNu7HtyiRDSYZGRkb6LEuSNJF+p25OqarhJIcA65M8VFXfHWfc24A7x0zb9LVtVa0GVgN0Op2a5H5Iknro64y+qoab+yeANcBJPYZewJhpm0lsK0maBhMGfZIFSfbbvgwsBx4YZ9wrgNOAWya7rSRp+vQzdbMIWJNk+/gbquqOJKsAquqaZty5wLqqenaibaeqeEnSxFI1+6bDO51ODQ0NDboMSdprJNnQ63NKfjJWklrOoJekljPoJanlDHpJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklqOYNeklrOoJekljPoJanlDHpJajmDXpJazqCXpJYz6CWp5foK+iSPJbk/yb1Jdvox1yRvTPK7pv/eJB/r6jszycNJNie5dCqLlyRNbP4kxp5eVb/aRf8/VtVbuxuSzAO+ALwJ2ArcneTWqnpw8qVKknbHdE/dnARsrqpHq+p54GvAOdP8nJKkLv0GfQHrkmxIsrLHmDckuS/J7Un+vGk7HNjSNWZr0yZJmiH9Tt2cUlXDSQ4B1id5qKq+29V/D/DKqnomyZuBbwDHTKaQ5n8gKwEWL148mU0lSbvQ1xl9VQ03908Aaxidkunuf6qqnmmWbwNekuRgYBg4smvoEU3beM+xuqo6VdVZuHDhpHdEkjS+CYM+yYIk+21fBpYDD4wZc2iSNMsnNY/7a+Bu4JgkRyfZB7gAuHVqd0GStCv9TN0sAtY0OT4fuKGq7kiyCqCqrgFWAO9Lsg34A3BBVRWwLckHgLXAPOBLVfXjadgPSVIPGc3j2aXT6dTQ0E6X60uSekiyoao64/X5yVhJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklqOYNeklrOoJekljPoJanlDHpJarlZ+e2VSUaAx2fo6Q4GdvWj54NmfXvG+vaM9e2+ma7tlVU17q82zcqgn0lJhnp9tedsYH17xvr2jPXtvtlUm1M3ktRyBr0ktZxBD6sHXcAErG/PWN+esb7dN2tqm/Nz9JLUdp7RS1LLGfSS1HJzMuiTfDDJQ0l+nOSqHmPOTPJwks1JLp3B2j6eZDjJvc3tzT3GPZbk/mbM0CysbyDHr+v5/2OSSnJwj/4Xuvbh1llY34VJftLcLpzBuq5IsrE5LuuS/GmPcQM5fpOob8aPX5JPNbmyMcmaJPv3GDfzr92qmlM34HTgH4A/adYPGWfMPOCnwKuAfYD7gH85Q/V9HPhPfYx7DDh4AMdvwvoGefya5z8SWMvoh+7GPUbAMzN97PqtDzgQeLS5P6BZPmCGavvnXcv/HrhmNh2/fuob1PEDlgPzm+W/A/6ux7gZf+3OxTP69wFXVtVzAFX1xDhjTgI2V9WjVfU88DXgnBmscW836OP3X4APAbP1SoOJ6jsDWF9VT1bVb4D1wJkzUVhVPdW1uoBZdgz7rG8gx6+q1lXVtmb1LuCI6X7Ofs3FoP8z4N8k+UGS7yQ5cZwxhwNbuta3Nm0z5QPNn39fSnJAjzEFrEuyIcnKGawNJq5vYMcvyTnAcFXdN8HQlyYZSnJXkrfPQGlA3/UN9N9fkv+cZAvwLuBjPYYN5PhBX/UN+vUL8G7g9h59M/7anT8TTzLTkvwDcOg4XZcxus8HAq8HTgT+Z5JXVfM31Syo72rgCkb/MVwBfIbRfzRjnVJVw0kOAdYneaiqvjuL6ps2E9T3UUb/hJ7IK5vj9yrg/ya5v6p+Oovqmza7qq+qbqmqy4DLknwE+ABw+ThjB3L8JlHftJiotmbMZcA24PoeDzNtr91eWhn0VfXvevUleR9wcxPsP0zyR0a/fGika9gwo/Oo2x3RtE17fWNq/e/A/+7xGMPN/RNJ1jA6XTIl/1imoL6BHL8kxwFHA/cl2f689yQ5qap+OeYxth+/R5N8G1jC6PsKs6G+YeCNXetHAN+eitp2Vd84rgduY5wgHcTxm0R903b8JqotyV8DbwWW9Tp5nM7Xbi9zcermG4y+IUuSP2P0zcKx3zB3N3BMkqOT7ANcAMzIlQVJDutaPRd4YJwxC5Lst32Z0TPEncYNqj4GdPyq6v6qOqSqjqqqoxj9k33p2JBPckCSP2mWDwb+NfDgbKmP0Tdqlzd1HsDof9+1010fQJJjulbPAR4aZ8xAjl+/9TGg45fkTEbfezm7qn7fY8xgXrsz+c7vbLgxGuz/ozm49wD/tmn/U+C2rnFvBh5h9Czlshms7yvA/cBGRsPxsLH1MXo1y33N7cezrb5BHr8xtT5Gc3UD0AGubZb/otmH+5r798ym+pr1dwObm9tFM1jTTc1rYyPwv4DDZ9Px66e+QR2/5rm2APc2t2ua9oG/dv0KBElqubk4dSNJc4pBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLWfQS1LL/X8nQkN/brdR0wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import singleVis.trajectory as trajectory\n",
    "trajectory.draw_trajectory(embeddings[7450][4:],x_min=-6.0, x_max=-2.5, y_min=5.5, y_max=7.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth of dists change and direction change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape\n",
    "high_dists = np.zeros((50000, 10))\n",
    "for i in range(50000):\n",
    "    for j in range(10):\n",
    "        high_dists[i][j] = np.linalg.norm(all_data[i][j]-all_data[i][j+1])\n",
    "low_dists = np.zeros((50000, 10))\n",
    "for i in range(50000):\n",
    "    for j in range(10):\n",
    "        low_dists[i][j] = np.linalg.norm(embeddings[i][j]-embeddings[i][j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dists[7450],low_dists[7450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "high_directions = np.zeros((50000, 9))\n",
    "for i in range(50000):\n",
    "    for j in range(9):\n",
    "        high_directions[i][j] = 1 - distance.cosine(all_data[i][j]-all_data[i][j+1], all_data[i][j+1]-all_data[i][j+2])\n",
    "low_directions = np.zeros((50000, 9))\n",
    "for i in range(50000):\n",
    "    for j in range(9):\n",
    "        low_directions[i][j] = 1 - distance.cosine(embeddings[i][j]-embeddings[i][j+1],embeddings[i][j+1]-embeddings[i][j+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_directions[7450],low_directions[7450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation betweem high dim dists and low dim dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_data = data_provider.train_representation(9)\n",
    "curr_data = data_provider.train_representation(11)\n",
    "\n",
    "dists = np.linalg.norm(prev_data - curr_data, axis=1)\n",
    "\n",
    "prev_data = torch.from_numpy(prev_data).to(device=data_provider.DEVICE, dtype=torch.float)\n",
    "prev_embedding = trainer.model.encoder(prev_data).detach().cpu().numpy()\n",
    "\n",
    "curr_data = torch.from_numpy(curr_data).to(device=data_provider.DEVICE, dtype=torch.float)\n",
    "curr_embedding = trainer.model.encoder(curr_data).detach().cpu().numpy()\n",
    "\n",
    "embedding_dists = np.linalg.norm(prev_embedding-curr_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "corr = pearsonr(dists, embedding_dists)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa7a9f36e1a1e240450dbe9cc8f6d8df1d5301f36681fb271c44fdd883236b60"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('SV': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
